{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd874c62-8f76-4f9c-9fb8-5adb59290cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from itertools import product\n",
    "from custom_functions import custom_accuracy_score, custom_confusion_matrix, custom_classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ff92119-690f-4a60-bcbe-b7c8842988e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training dataset\n",
    "train_csv_path = './archive/sign_mnist_train.csv'\n",
    "train_data = pd.read_csv(train_csv_path)\n",
    "\n",
    "# Load the testing dataset\n",
    "test_csv_path = './archive/sign_mnist_test.csv'\n",
    "test_data = pd.read_csv(test_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a75e225-6a12-4877-931b-ac4680d952ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000, num_classes=None):\n",
    "        \"\"\"\n",
    "        Initialize the Logistic Regression model\n",
    "        \n",
    "        Parameters:\n",
    "        - learning_rate: step size for gradient descent\n",
    "        - num_iterations: number of training iterations\n",
    "        - num_classes: number of unique classes in the dataset\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.num_classes = num_classes\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.class_mapping = None  # To handle zero-based indexing\n",
    "\n",
    "    def _softmax(self, z):\n",
    "        \"\"\"\n",
    "        Softmax activation function for multiclass classification\n",
    "        Prevents numerical instability by subtracting max value\n",
    "        \n",
    "        Parameters:\n",
    "        - z: input array of logits\n",
    "        \n",
    "        Returns:\n",
    "        - Softmax probabilities\n",
    "        \"\"\"\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def _one_hot_encode(self, y):\n",
    "        \"\"\"\n",
    "        Convert labels to one-hot encoded format\n",
    "        \n",
    "        Parameters:\n",
    "        - y: original labels\n",
    "        \n",
    "        Returns:\n",
    "        - One-hot encoded labels\n",
    "        \"\"\"\n",
    "        # Remap labels to zero-based index if needed\n",
    "        if self.class_mapping is None:\n",
    "            unique_classes = np.unique(y)\n",
    "            self.class_mapping = {orig: idx for idx, orig in enumerate(unique_classes)}\n",
    "            self.reverse_mapping = {idx: orig for orig, idx in self.class_mapping.items()}\n",
    "        \n",
    "        # Map original labels to zero-based index\n",
    "        y_mapped = np.array([self.class_mapping[label] for label in y])\n",
    "        \n",
    "        # Create one-hot encoding\n",
    "        one_hot = np.zeros((y.shape[0], self.num_classes))\n",
    "        one_hot[np.arange(y.shape[0]), y_mapped] = 1\n",
    "        return one_hot\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the logistic regression model\n",
    "        \n",
    "        Parameters:\n",
    "        - X: input features (num_samples, num_features)\n",
    "        - y: target labels\n",
    "        \"\"\"\n",
    "        # Determine number of classes if not specified\n",
    "        unique_classes = np.unique(y)\n",
    "        self.num_classes = len(unique_classes)\n",
    "        \n",
    "        # Initialize weights and bias\n",
    "        num_features = X.shape[1]\n",
    "        self.weights = np.zeros((num_features, self.num_classes))\n",
    "        self.bias = np.zeros((1, self.num_classes))\n",
    "        \n",
    "        # One-hot encode labels\n",
    "        Y_one_hot = self._one_hot_encode(y)\n",
    "        \n",
    "        # Gradient descent\n",
    "        for _ in range(self.num_iterations):\n",
    "            # Forward pass\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self._softmax(linear_model)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw = (1/X.shape[0]) * np.dot(X.T, (y_predicted - Y_one_hot))\n",
    "            db = (1/X.shape[0]) * np.sum(y_predicted - Y_one_hot, axis=0, keepdims=True)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on input data\n",
    "        \n",
    "        Parameters:\n",
    "        - X: input features\n",
    "        \n",
    "        Returns:\n",
    "        - Predicted class labels (original class labels)\n",
    "        \"\"\"\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self._softmax(linear_model)\n",
    "        \n",
    "        # Get indices of max probabilities\n",
    "        predicted_indices = np.argmax(y_predicted, axis=1)\n",
    "        \n",
    "        # Map back to original class labels\n",
    "        return np.array([self.reverse_mapping[idx] for idx in predicted_indices])\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities\n",
    "        \n",
    "        Parameters:\n",
    "        - X: input features\n",
    "        \n",
    "        Returns:\n",
    "        - Predicted class probabilities\n",
    "        \"\"\"\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        return self._softmax(linear_model)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute model accuracy\n",
    "        \n",
    "        Parameters:\n",
    "        - X: input features\n",
    "        - y: true labels\n",
    "        \n",
    "        Returns:\n",
    "        - Accuracy score\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "\n",
    "def preprocess_sign_mnist(train_data, test_data):\n",
    "    \"\"\"\n",
    "    Preprocess Sign MNIST dataset\n",
    "    \n",
    "    Parameters:\n",
    "    - train_data: Training dataframe\n",
    "    - test_data: Testing dataframe\n",
    "    \n",
    "    Returns:\n",
    "    - Preprocessed X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    # Separate features and labels\n",
    "    X_train = train_data.drop('label', axis=1).values\n",
    "    y_train = train_data['label'].values\n",
    "    \n",
    "    X_test = test_data.drop('label', axis=1).values\n",
    "    y_test = test_data['label'].values\n",
    "    \n",
    "    # Normalize pixel values to [0, 1]\n",
    "    X_train = X_train.astype('float32') / 255.0\n",
    "    X_test = X_test.astype('float32') / 255.0\n",
    "    \n",
    "    print(\"Unique training classes:\", np.unique(y_train))\n",
    "    print(\"Unique testing classes:\", np.unique(y_test))\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_logistic_regression(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train custom Logistic Regression on Sign MNIST\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Training features\n",
    "    - X_test: Testing features\n",
    "    - y_train: Training labels\n",
    "    - y_test: Testing labels\n",
    "    \n",
    "    Returns:\n",
    "    - Trained model\n",
    "    - Training and test accuracies\n",
    "    \"\"\"\n",
    "    # Create and train the model\n",
    "    clf = CustomLogisticRegression(\n",
    "        learning_rate=0.1,  # You can tune this\n",
    "        num_iterations=1000,  # You can increase for better convergence\n",
    "        num_classes=len(np.unique(y_train))\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Compute accuracies\n",
    "    train_accuracy = clf.accuracy(X_train, y_train)\n",
    "    test_accuracy = clf.accuracy(X_test, y_test)\n",
    "    \n",
    "    return clf, train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e66ec64-5f68-4598-854d-c4e6bac43d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sign_mnist(train_data, test_data):\n",
    "    # Separate features and labels\n",
    "    X_train = train_data.drop('label', axis=1).values\n",
    "    y_train = train_data['label'].values\n",
    "    \n",
    "    X_test = test_data.drop('label', axis=1).values\n",
    "    y_test = test_data['label'].values\n",
    "    \n",
    "    # Normalize pixel values to [0, 1]\n",
    "    X_train = X_train.astype('float32') / 255.0\n",
    "    X_test = X_test.astype('float32') / 255.0\n",
    "    \n",
    "    print(\"Unique training classes:\", np.unique(y_train))\n",
    "    print(\"Unique testing classes:\", np.unique(y_test))\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c2e8240-c8a8-4a13-828a-14d951e934a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique training classes: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n",
      "Unique testing classes: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "X_train, X_test, y_train, y_test = preprocess_sign_mnist(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd075c60-e052-4056-8015-b6705f92e653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "clf = CustomLogisticRegression(\n",
    "    learning_rate=0.1, \n",
    "    num_iterations=1000,  \n",
    "    num_classes=len(np.unique(y_train))\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485248d3-6425-49d7-9059-a7827bb7f2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracies\n",
    "train_accuracy = clf.accuracy(X_train, y_train)\n",
    "test_accuracy = clf.accuracy(X_test, y_test)\n",
    "\n",
    "# Evaluation metrics using custom implementations\n",
    "custom_accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Testing Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1444857a-3715-45cd-96f8-2f56bbf0b193",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "custom_confusion_matrix(y_true=y_test, y_pred=y_pred, num_classes=len(np.unique(y_train)))\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "custom_classification_report(y_test, y_pred, num_classes=len(np.unique(y_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53ce136-6cdb-420e-9565-85470596e73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some predictions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select a few random test samples\n",
    "num_samples_to_show = 5\n",
    "indices = np.random.randint(0, X_test.shape[0], num_samples_to_show)\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i, idx in enumerate(indices):\n",
    "    plt.subplot(1, num_samples_to_show, i+1)\n",
    "    plt.imshow(X_test[idx].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"Pred: {y_pred[idx]}, True: {y_test[idx]}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a28610-c3a4-44a3-9298-e2e7dc98cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLogisticRegressionWithReg(CustomLogisticRegression):\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000, num_classes=None, reg_strength=0.01):\n",
    "        super().__init__(learning_rate, num_iterations, num_classes)\n",
    "        self.reg_strength = reg_strength  # L2 Regularization strength\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        unique_classes = np.unique(y)\n",
    "        self.num_classes = len(unique_classes)\n",
    "        num_features = X.shape[1]\n",
    "        self.weights = np.zeros((num_features, self.num_classes))\n",
    "        self.bias = np.zeros((1, self.num_classes))\n",
    "        Y_one_hot = self._one_hot_encode(y)\n",
    "        \n",
    "        for _ in range(self.num_iterations):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self._softmax(linear_model)\n",
    "            dw = (1 / X.shape[0]) * np.dot(X.T, (y_predicted - Y_one_hot)) + self.reg_strength * self.weights\n",
    "            db = (1 / X.shape[0]) * np.sum(y_predicted - Y_one_hot, axis=0, keepdims=True)\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec823b93-5fae-48cd-9dff-c4bb09388ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(X_train, y_train, X_test, y_test):\n",
    "    learning_rates = [0.01, 0.05, 0.1]\n",
    "    num_iterations_list = [500, 1000, 2000]\n",
    "    reg_strengths = [0.01, 0.1, 1.0]\n",
    "\n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "    best_params = None\n",
    "\n",
    "    for lr, num_iter, reg in product(learning_rates, num_iterations_list, reg_strengths):\n",
    "        print(f\"Training with learning_rate={lr}, num_iterations={num_iter}, reg_strength={reg}\")\n",
    "        model = CustomLogisticRegressionWithReg(learning_rate=lr, num_iterations=num_iter, reg_strength=reg)\n",
    "        model.fit(X_train, y_train)\n",
    "        accuracy = model.accuracy(X_test, y_test)\n",
    "        print(f\"Testing accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_model = model\n",
    "            best_accuracy = accuracy\n",
    "            best_params = (lr, num_iter, reg)\n",
    "\n",
    "    print(\"\\nBest Hyperparameters:\")\n",
    "    print(f\"Learning Rate: {best_params[0]}, Number of Iterations: {best_params[1]}, Regularization Strength: {best_params[2]}\")\n",
    "    print(f\"Best Testing Accuracy: {best_accuracy * 100:.2f}%\")\n",
    "    return best_model, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f19e5c5-b45f-4564-a955-cb08b17e0e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "X_train, X_test, y_train, y_test = preprocess_sign_mnist(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da0788b2-77b9-461c-bf82-8cd6c6ed4d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique training classes: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n",
      "Unique testing classes: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n",
      "Starting hyperparameter tuning...\n",
      "\n",
      "Training with learning_rate=0.01, num_iterations=500, reg_strength=0.01\n",
      "Testing accuracy: 39.75%\n",
      "Training with learning_rate=0.01, num_iterations=500, reg_strength=0.1\n",
      "Testing accuracy: 37.95%\n",
      "Training with learning_rate=0.01, num_iterations=500, reg_strength=1.0\n",
      "Testing accuracy: 20.58%\n",
      "Training with learning_rate=0.01, num_iterations=1000, reg_strength=0.01\n",
      "Testing accuracy: 45.96%\n",
      "Training with learning_rate=0.01, num_iterations=1000, reg_strength=0.1\n",
      "Testing accuracy: 42.68%\n",
      "Training with learning_rate=0.01, num_iterations=1000, reg_strength=1.0\n",
      "Testing accuracy: 20.84%\n",
      "Training with learning_rate=0.01, num_iterations=2000, reg_strength=0.01\n",
      "Testing accuracy: 51.44%\n",
      "Training with learning_rate=0.01, num_iterations=2000, reg_strength=0.1\n",
      "Testing accuracy: 46.01%\n",
      "Training with learning_rate=0.01, num_iterations=2000, reg_strength=1.0\n",
      "Testing accuracy: 21.24%\n",
      "Training with learning_rate=0.05, num_iterations=500, reg_strength=0.01\n",
      "Testing accuracy: 53.40%\n",
      "Training with learning_rate=0.05, num_iterations=500, reg_strength=0.1\n",
      "Testing accuracy: 46.47%\n",
      "Training with learning_rate=0.05, num_iterations=500, reg_strength=1.0\n",
      "Testing accuracy: 21.43%\n",
      "Training with learning_rate=0.05, num_iterations=1000, reg_strength=0.01\n",
      "Testing accuracy: 55.67%\n",
      "Training with learning_rate=0.05, num_iterations=1000, reg_strength=0.1\n",
      "Testing accuracy: 46.89%\n",
      "Training with learning_rate=0.05, num_iterations=1000, reg_strength=1.0\n",
      "Testing accuracy: 22.04%\n",
      "Training with learning_rate=0.05, num_iterations=2000, reg_strength=0.01\n",
      "Testing accuracy: 59.36%\n",
      "Training with learning_rate=0.05, num_iterations=2000, reg_strength=0.1\n",
      "Testing accuracy: 46.79%\n",
      "Training with learning_rate=0.05, num_iterations=2000, reg_strength=1.0\n",
      "Testing accuracy: 21.68%\n",
      "Training with learning_rate=0.1, num_iterations=500, reg_strength=0.01\n",
      "Testing accuracy: 55.67%\n",
      "Training with learning_rate=0.1, num_iterations=500, reg_strength=0.1\n",
      "Testing accuracy: 46.89%\n",
      "Training with learning_rate=0.1, num_iterations=500, reg_strength=1.0\n",
      "Testing accuracy: 22.04%\n",
      "Training with learning_rate=0.1, num_iterations=1000, reg_strength=0.01\n",
      "Testing accuracy: 59.36%\n",
      "Training with learning_rate=0.1, num_iterations=1000, reg_strength=0.1\n",
      "Testing accuracy: 46.79%\n",
      "Training with learning_rate=0.1, num_iterations=1000, reg_strength=1.0\n",
      "Testing accuracy: 21.68%\n",
      "Training with learning_rate=0.1, num_iterations=2000, reg_strength=0.01\n",
      "Testing accuracy: 62.98%\n",
      "Training with learning_rate=0.1, num_iterations=2000, reg_strength=0.1\n",
      "Testing accuracy: 46.58%\n",
      "Training with learning_rate=0.1, num_iterations=2000, reg_strength=1.0\n",
      "Testing accuracy: 19.97%\n",
      "\n",
      "Best Hyperparameters:\n",
      "Learning Rate: 0.1, Number of Iterations: 2000, Regularization Strength: 0.01\n",
      "Best Testing Accuracy: 62.98%\n",
      "\n",
      "Evaluating the best model with optimal hyperparameters...\n",
      "Confusion Matrix:\n",
      "[[327   0   0   0   0   0   0   0   0   0   0   0   1   3   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0 349   0   0   0   0   0   0   0  83   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0 273   0   0  14   0   0   0   0   9   0   0   8   0   0   0   0\n",
      "    0   0   0   0   6   0]\n",
      " [  0   0   0 140   0   0   0   0   0  28   0   0  12   0   0   0   0   5\n",
      "    0   0   4   0  56   0]\n",
      " [  0   0   0   0 435   0   0   0   0   0   0   0   0   0   0   0   0  63\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0  19   0   0 206   0   0   0   0   2   0   0   0   0   0   0   0\n",
      "    0   0   0  20   0   0]\n",
      " [  0   0   0  20   0   0 201  17   0   0   0   0  19  13   0  21   0   0\n",
      "   56   0   0   0   1   0]\n",
      " [  0   0   0   0   0   0  76 324   0   0   0   0   0   0   0   0   0   0\n",
      "   29   0   0   0   7   0]\n",
      " [  5   0   0   0   0   0   0   0 182   0   0   0  21   0   0  18   0  21\n",
      "    0   0   0   0   0  41]\n",
      " [  0   0   0  10   0  21   0   0  21 166   0   0   0   0   0   0  87   5\n",
      "    0   0   0   1   0  20]\n",
      " [  0   0   0   0   0   0   0   0   0   0 209   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [ 33   0   0   0  54   0   0   0   0   0   0 122  46   0   0  41   0  98\n",
      "    0   0   0   0   0   0]\n",
      " [ 42   0   0   4  28   0   0   0   0   0   0   1 130   8   0  34   0  10\n",
      "   34   0   0   0   0   0]\n",
      " [  0   0  15   0  21  21   8   0   0   0   0   0   0 139   0  22   0   0\n",
      "    8   4   0   0   8   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 325  21   0   0\n",
      "    1   0   0   0   0   0]\n",
      " [  0   0   0   0   0   3   0   0   0   0   0  21   0   0   0 140   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  21   0   0   0   0   0  41  62\n",
      "    0  20   0   0   0   0]\n",
      " [  0   0   0   2  23   0   0   0  41   0   0  41  25   0   0   4   0  71\n",
      "    0   1   0  17   0  21]\n",
      " [  0   0   1   0   0   0   0   0  21   0  31   0   0   0  21   0   0   0\n",
      "  114  20   0   0  40   0]\n",
      " [  0  16   0  40   0   4   0   0   0  41   0   0   0   0   0   0  62   0\n",
      "    0  82  21   0   0   0]\n",
      " [  0   6   0   3   0  32   0   0   0  18  12   0   0   0  21   0  44   0\n",
      "    5  53 115  36   0   1]\n",
      " [  0  20   0   0   0   8   0   0   0  20   0   0   0   0   0   0  38   0\n",
      "    0  12   0 108   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   9  24\n",
      "   21   0   0  46 167   0]\n",
      " [  0   0   0  17   0   0   0   0  12   0   3   0   0   0   0   0  78  14\n",
      "   42   0   0  15   0 151]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.99      0.89       331\n",
      "           1       0.89      0.81      0.85       432\n",
      "           2       0.89      0.88      0.88       310\n",
      "           3       0.59      0.57      0.58       245\n",
      "           4       0.78      0.87      0.82       498\n",
      "           5       0.67      0.83      0.74       247\n",
      "           6       0.71      0.58      0.64       348\n",
      "           7       0.95      0.74      0.83       436\n",
      "           8       0.66      0.63      0.64       288\n",
      "          10       0.47      0.50      0.48       331\n",
      "          11       0.73      1.00      0.84       209\n",
      "          12       0.66      0.31      0.42       394\n",
      "          13       0.51      0.45      0.48       291\n",
      "          14       0.81      0.57      0.67       246\n",
      "          15       0.89      0.94      0.91       347\n",
      "          16       0.47      0.85      0.60       164\n",
      "          17       0.11      0.28      0.16       144\n",
      "          18       0.19      0.29      0.23       246\n",
      "          19       0.37      0.46      0.41       248\n",
      "          20       0.43      0.31      0.36       266\n",
      "          21       0.82      0.33      0.47       346\n",
      "          22       0.44      0.52      0.48       206\n",
      "          23       0.59      0.63      0.61       267\n",
      "          24       0.65      0.45      0.53       332\n",
      "\n",
      "    accuracy                           0.63      7172\n",
      "   macro avg       0.63      0.62      0.61      7172\n",
      "weighted avg       0.67      0.63      0.63      7172\n",
      "\n",
      "\n",
      "Summary of Best Model:\n",
      "Best Hyperparameters:\n",
      "  Learning Rate: 0.1\n",
      "  Number of Iterations: 2000\n",
      "  Regularization Strength: 0.01\n",
      "Best Testing Accuracy: 62.98%\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "X_train, X_test, y_train, y_test = preprocess_sign_mnist(train_data, test_data)\n",
    "\n",
    "# Hyperparameter tuning with detailed output\n",
    "print(\"Starting hyperparameter tuning...\\n\")\n",
    "best_model, best_params = tune_hyperparameters(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "print(\"\\nEvaluating the best model with optimal hyperparameters...\")\n",
    "evaluate_model(best_model, X_test, y_test)\n",
    "\n",
    "# Print the final results\n",
    "print(\"\\nSummary of Best Model:\")\n",
    "print(f\"Best Hyperparameters:\\n  Learning Rate: {best_params[0]}\\n  Number of Iterations: {best_params[1]}\\n  Regularization Strength: {best_params[2]}\")\n",
    "print(f\"Best Testing Accuracy: {best_model.accuracy(X_test, y_test) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfddd51-0e28-4938-9c48-a9a26314ee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def tune_hyperparameters_generic(X_train, y_train, X_test, y_test, \n",
    "                                  learning_rates, num_iterations_list, reg_strengths):\n",
    "    \"\"\"\n",
    "    Generic hyperparameter tuning function for logistic regression.\n",
    "\n",
    "    Parameters:\n",
    "        X_train, y_train: Training data and labels\n",
    "        X_test, y_test: Testing data and labels\n",
    "        learning_rates: List of learning rates to try\n",
    "        num_iterations_list: List of iteration counts to try\n",
    "        reg_strengths: List of regularization strengths to try\n",
    "\n",
    "    Returns:\n",
    "        best_model: Trained model with the best parameters\n",
    "        best_params: Tuple of (learning_rate, num_iterations, reg_strength) for the best model\n",
    "    \"\"\"\n",
    "    print(\"Starting hyperparameter tuning...\\n\")\n",
    "    \n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "    best_params = None\n",
    "\n",
    "    # Iterate through all combinations of hyperparameters\n",
    "    for lr, num_iter, reg in product(learning_rates, num_iterations_list, reg_strengths):\n",
    "        print(f\"Training with learning_rate={lr}, num_iterations={num_iter}, reg_strength={reg}\")\n",
    "        model = CustomLogisticRegressionWithReg(learning_rate=lr, num_iterations=num_iter, reg_strength=reg)\n",
    "        model.fit(X_train, y_train)\n",
    "        accuracy = model.accuracy(X_test, y_test)\n",
    "        print(f\"Testing accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "\n",
    "        # Check for the best model\n",
    "        if accuracy > best_accuracy:\n",
    "            best_model = model\n",
    "            best_accuracy = accuracy\n",
    "            best_params = (lr, num_iter, reg)\n",
    "\n",
    "    # Final summary of the best model\n",
    "    print(\"\\nEvaluating the best model with optimal hyperparameters..................\")\n",
    "    evaluate_model(best_model, X_test, y_test)\n",
    "\n",
    "    print(\"\\nSummary of Best Model:\")\n",
    "    print(f\"Best Hyperparameters:\\n  Learning Rate: {best_params[0]}\\n  Number of Iterations: {best_params[1]}\\n  Regularization Strength: {best_params[2]}\")\n",
    "    print(f\"Best Testing Accuracy: {best_accuracy * 100:.2f}%\")\n",
    "\n",
    "    return best_model, best_params\n",
    "\n",
    "# Example usage:\n",
    "# Define hyperparameter ranges\n",
    "learning_rates = [0.01, 0.02, 0.03, 0.05]\n",
    "num_iterations_list = [2000, 3000, 5000]\n",
    "reg_strengths = [0.005, 0.01, 0.02]\n",
    "\n",
    "\n",
    "# learning_rates = [0.005, 0.01, 0.02, 0.03, 0.05]\n",
    "# num_iterations_list = [2000, 3000, 5000, 7000]\n",
    "# reg_strengths = [0.005, 0.01, 0.02, 0.03, 0.04]\n",
    "\n",
    "\n",
    "# Preprocess the data\n",
    "X_train, X_test, y_train, y_test = preprocess_sign_mnist(train_data, test_data)\n",
    "\n",
    "# Call the generic tuning function\n",
    "best_model, best_params = tune_hyperparameters_generic(\n",
    "    X_train, y_train, X_test, y_test, \n",
    "    learning_rates, num_iterations_list, reg_strengths\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fd1efd-d9b8-463a-9742-f79de754667b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
