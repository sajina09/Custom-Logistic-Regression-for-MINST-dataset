{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c49bdd00-11c6-4e76-a0f5-383e62af33cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from custom_functions import custom_accuracy_score, custom_confusion_matrix, custom_classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bdd1797-bb72-4e93-b401-59410bddd3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training dataset\n",
    "train_csv_path = './archive/sign_mnist_train.csv'\n",
    "train_data = pd.read_csv(train_csv_path)\n",
    "\n",
    "# Load the testing dataset\n",
    "test_csv_path = './archive/sign_mnist_test.csv'\n",
    "test_data = pd.read_csv(test_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd6e2b70-224b-4d28-873e-d4375c06b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "X_train = train_data.drop('label', axis=1).values\n",
    "y_train = train_data['label'].values\n",
    "\n",
    "X_test = test_data.drop('label', axis=1).values\n",
    "y_test = test_data['label'].values\n",
    "\n",
    "# Normalize pixel values\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eda4a0e6-c007-4e4b-819a-93489d828783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique training classes: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n",
      "Unique testing classes: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_sign_mnist(train_data, test_data):\n",
    "    # Separate features and labels\n",
    "    X_train = train_data.drop('label', axis=1).values\n",
    "    y_train = train_data['label'].values\n",
    "\n",
    "    X_test = test_data.drop('label', axis=1).values\n",
    "    y_test = test_data['label'].values\n",
    "\n",
    "    # Normalize pixel values to [0, 1]\n",
    "    X_train = X_train.astype('float32') / 255.0\n",
    "    X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "    print(\"Unique training classes:\", np.unique(y_train))\n",
    "    print(\"Unique testing classes:\", np.unique(y_test))\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Preprocess the data\n",
    "X_train, X_test, y_train, y_test = preprocess_sign_mnist(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8e2a743-c03f-419d-b79d-1c6deb0b930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSVM:\n",
    "    def __init__(self, C=1, kernel='linear', degree=3, gamma='auto'):\n",
    "        \"\"\"\n",
    "        Initialize the Custom SVM Classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        - C (float): Penalty parameter of the error term.\n",
    "        - kernel (str): Kernel function to use ('linear', 'poly', 'rbf').\n",
    "        - degree (int): Degree of the polynomial kernel.\n",
    "        - gamma (str or float): Kernel coefficient for 'rbf' and 'poly' kernels.\n",
    "        \"\"\"\n",
    "        self.C = C\n",
    "        self.kernel = kernel\n",
    "        self.degree = degree\n",
    "        self.gamma = gamma\n",
    "        self.alpha = None\n",
    "        self.support_vectors = None\n",
    "        self.support_vector_labels = None\n",
    "        self.intercept = None\n",
    "\n",
    "    def _kernel(self, X1, X2):\n",
    "        \"\"\"\n",
    "        Compute the kernel function between two sets of samples.\n",
    "        \n",
    "        Parameters:\n",
    "        - X1 (numpy.ndarray): First set of samples.\n",
    "        - X2 (numpy.ndarray): Second set of samples.\n",
    "        \n",
    "        Returns:\n",
    "        - Kernel matrix.\n",
    "        \"\"\"\n",
    "        if self.kernel == 'linear':\n",
    "            return np.dot(X1, X2.T)\n",
    "        elif self.kernel == 'poly':\n",
    "            return (1 + np.dot(X1, X2.T)) ** self.degree\n",
    "        elif self.kernel == 'rbf':\n",
    "            if self.gamma == 'auto':\n",
    "                self.gamma = 1 / X1.shape[1]\n",
    "            return np.exp(-self.gamma * np.sum((X1[:, None] - X2) ** 2, axis=-1))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the Custom SVM Classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        - X (numpy.ndarray): Training samples.\n",
    "        - y (numpy.ndarray): Training labels.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize Lagrange multipliers\n",
    "        self.alpha = np.zeros(n_samples)\n",
    "        \n",
    "        # Compute kernel matrix\n",
    "        K = self._kernel(X, X)\n",
    "        \n",
    "        # Optimize Lagrange multipliers\n",
    "        for _ in range(1000):\n",
    "            for i in range(n_samples):\n",
    "                grad = np.sum(self.alpha * y * K[:, i]) - 1\n",
    "                if (y[i] * grad < -1e-5) and (self.alpha[i] < self.C):\n",
    "                    self.alpha[i] += y[i]\n",
    "        \n",
    "        # Identify support vectors\n",
    "        self.support_vectors = X[self.alpha > 1e-5]\n",
    "        self.support_vector_labels = y[self.alpha > 1e-5]\n",
    "        \n",
    "        # Compute intercept\n",
    "        self.intercept = np.mean(self.support_vector_labels -\n",
    "                                np.dot(self._kernel(self.support_vectors, self.support_vectors),\n",
    "                                       self.support_vector_labels * self.alpha[self.alpha > 1e-5]))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions using the trained Custom SVM Classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        - X (numpy.ndarray): Samples to predict.\n",
    "        \n",
    "        Returns:\n",
    "        - Predicted labels.\n",
    "        \"\"\"\n",
    "        decision_values = np.dot(self._kernel(X, self.support_vectors),\n",
    "                                self.support_vector_labels * self.alpha[self.alpha > 1e-5]) + self.intercept\n",
    "        return np.sign(decision_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba35239-d541-42ac-82e1-19a29f175e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the Custom SVM model\n",
    "svm = CustomSVM(C=1, kernel='linear', gamma='auto', degree=3)  # Set appropriate parameters\n",
    "svm.fit(X_train, y_train)  # Train the model\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluation metrics using custom implementations\n",
    "accuracy = custom_accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the model calculated : {accuracy:.2f}\")\n",
    "custom_confusion_matrix(y_true=y_test, y_pred=y_pred, num_classes=len(np.unique(y_train)))\n",
    "custom_classification_report(y_test, y_pred, num_classes=len(np.unique(y_train)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb3925-de2b-4cbb-a348-10ad4ec49599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypertune_custom_svm(X_train, y_train, X_test, y_test, param_grid):\n",
    "    \"\"\"\n",
    "    Hypertune the parameters of the Custom SVM implementation with progress updates.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train, y_train: Training data and labels\n",
    "        X_test, y_test: Testing data and labels\n",
    "        param_grid: Dictionary of parameters to hypertune\n",
    "        \n",
    "    Returns:\n",
    "        best_params: Best combination of parameters\n",
    "        best_accuracy: Highest accuracy achieved\n",
    "    \"\"\"\n",
    "    best_params = None\n",
    "    best_accuracy = 0\n",
    "    results = []\n",
    "    total_combinations = (len(param_grid['C']) *\n",
    "                          len(param_grid['kernel']) *\n",
    "                          len(param_grid['gamma']) *\n",
    "                          len(param_grid['degree']))\n",
    "    progress = 0  # Track the current progress\n",
    "\n",
    "    print(f\"Total combinations to test: {total_combinations}\\n\")\n",
    "\n",
    "    for C in param_grid['C']:\n",
    "        for kernel in param_grid['kernel']:\n",
    "            for gamma in param_grid['gamma']:\n",
    "                for degree in param_grid['degree']:\n",
    "                    progress += 1\n",
    "                    print(f\"Testing combination {progress}/{total_combinations}: C={C}, kernel={kernel}, gamma={gamma}, degree={degree}\")\n",
    "                    \n",
    "                    # Train the Custom SVM\n",
    "                    svm = CustomSVM(C=C, kernel=kernel, degree=degree, gamma=gamma)\n",
    "                    svm.fit(X_train, y_train)\n",
    "                    \n",
    "                    # Predict on the test set\n",
    "                    y_pred = svm.predict(X_test)\n",
    "                    \n",
    "                    # Evaluate accuracy\n",
    "                    accuracy = custom_accuracy_score(y_test, y_pred)\n",
    "                    print(f\"Accuracy for this combination: {accuracy:.4f}\\n\")\n",
    "                    \n",
    "                    # Store results\n",
    "                    results.append((C, kernel, gamma, degree, accuracy))\n",
    "                    \n",
    "                    # Update best parameters if current is better\n",
    "                    if accuracy > best_accuracy:\n",
    "                        best_accuracy = accuracy\n",
    "                        best_params = {'C': C, 'kernel': kernel, 'gamma': gamma, 'degree': degree}\n",
    "\n",
    "                    # Print progress every 10% of combinations\n",
    "                    if progress % (total_combinations // 10) == 0 or progress == total_combinations:\n",
    "                        print(f\"Progress: {progress}/{total_combinations} combinations tested.\")\n",
    "    \n",
    "    print(\"\\nBest Parameters Found:\")\n",
    "    print(best_params)\n",
    "    print(f\"Best Accuracy: {best_accuracy:.4f}\")\n",
    "    \n",
    "    return best_params, best_accuracy, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c518c9-663f-41a5-b54e-7af48cec0b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
