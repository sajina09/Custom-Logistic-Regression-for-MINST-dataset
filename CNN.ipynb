{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cede69c6-1488-4bf2-b7dc-b63aab36a09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ea8713d-7083-4389-8dc7-b8b61cba799a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "318016f0-7b17-4fcf-8750-2d8f9911cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training dataset\n",
    "train_csv_path = './archive/sign_mnist_train.csv'\n",
    "train_data = pd.read_csv(train_csv_path)\n",
    "\n",
    "# Load the testing dataset\n",
    "test_csv_path = './archive/sign_mnist_test.csv'\n",
    "test_data = pd.read_csv(test_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4710817-50f7-43be-8061-d9de85787548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels for training\n",
    "X_train = train_data.iloc[:, 1:].values  # Pixel values\n",
    "Y_train = train_data.iloc[:, 0].values  # Labels\n",
    "\n",
    "# Separate features and labels for testing\n",
    "X_test = test_data.iloc[:, 1:].values\n",
    "Y_test = test_data.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92048788-0031-4be9-ae6e-b0a51f1bdb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the pixel values (scale to 0-1)\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "938cf610-2729-4271-a8f5-9198a4c597ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Stability trick\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "def cross_entropy_derivative(y_true, y_pred):\n",
    "    return y_pred - y_true\n",
    "\n",
    "# Convolutional Layer\n",
    "class ConvLayer:\n",
    "    def __init__(self, num_filters, filter_size):\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_size = filter_size\n",
    "        self.filters = np.random.randn(num_filters, filter_size, filter_size) * 0.1  # Initialize filters\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        h, w = X.shape[1], X.shape[2]\n",
    "        output_height = h - self.filter_size + 1\n",
    "        output_width = w - self.filter_size + 1\n",
    "\n",
    "        self.output = np.zeros((X.shape[0], self.num_filters, output_height, output_width))\n",
    "        for i in range(self.num_filters):\n",
    "            for j in range(output_height):\n",
    "                for k in range(output_width):\n",
    "                    region = X[:, j:j + self.filter_size, k:k + self.filter_size]\n",
    "                    self.output[:, i, j, k] = np.sum(region * self.filters[i], axis=(1, 2))\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dL_dout, learning_rate):\n",
    "        dL_dfilters = np.zeros_like(self.filters)\n",
    "        for i in range(self.num_filters):\n",
    "            for j in range(dL_dout.shape[2]):\n",
    "                for k in range(dL_dout.shape[3]):\n",
    "                    region = self.input[:, j:j + self.filter_size, k:k + self.filter_size]\n",
    "                    dL_dfilters[i] += np.sum(dL_dout[:, i, j, k][:, None, None] * region, axis=0)\n",
    "\n",
    "        # Update filters\n",
    "        self.filters -= learning_rate * dL_dfilters\n",
    "        return None  # Pooling and later layers will handle further backpropagation\n",
    "\n",
    "# MaxPooling Layer\n",
    "class MaxPoolLayer:\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        h, w = X.shape[2], X.shape[3]\n",
    "        output_height = h // self.pool_size\n",
    "        output_width = w // self.pool_size\n",
    "\n",
    "        self.output = np.zeros((X.shape[0], X.shape[1], output_height, output_width))\n",
    "        for i in range(output_height):\n",
    "            for j in range(output_width):\n",
    "                region = X[:, :, i * self.pool_size:(i + 1) * self.pool_size,\n",
    "                            j * self.pool_size:(j + 1) * self.pool_size]\n",
    "                self.output[:, :, i, j] = np.max(region, axis=(2, 3))\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dL_dout):\n",
    "        return None  # Not required in this simplified implementation\n",
    "\n",
    "# Fully Connected Layer\n",
    "class FullyConnectedLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.1\n",
    "        self.biases = np.zeros(output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        return np.dot(X, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dL_dout, learning_rate):\n",
    "        dL_dweights = np.dot(self.input.T, dL_dout)\n",
    "        dL_dbiases = np.sum(dL_dout, axis=0)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights -= learning_rate * dL_dweights\n",
    "        self.biases -= learning_rate * dL_dbiases\n",
    "\n",
    "        # Return gradient for the next layer\n",
    "        return np.dot(dL_dout, self.weights.T)\n",
    "\n",
    "# CNN Model\n",
    "class CNN:\n",
    "    def __init__(self):\n",
    "        self.conv1 = ConvLayer(num_filters=8, filter_size=3)\n",
    "        self.pool1 = MaxPoolLayer(pool_size=2)\n",
    "        self.fc = FullyConnectedLayer(input_size=13 * 13 * 8, output_size=25)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.conv1.forward(X)\n",
    "        X = relu(X)\n",
    "        X = self.pool1.forward(X)\n",
    "        X = X.reshape(X.shape[0], -1)  # Flatten\n",
    "        X = self.fc.forward(X)\n",
    "        return softmax(X)\n",
    "\n",
    "    def backward(self, X, y, y_pred, learning_rate):\n",
    "        dL_dout = cross_entropy_derivative(y, y_pred)\n",
    "        dL_dout = self.fc.backward(dL_dout, learning_rate)\n",
    "        # Skipping backpropagation through pool1 and conv1 for simplicity\n",
    "\n",
    "    def train(self, X_train, y_train, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X_train)\n",
    "            loss = cross_entropy_loss(y_train, y_pred)\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss:.4f}\")\n",
    "            self.backward(X_train, y_train, y_pred, learning_rate)\n",
    "\n",
    "def preprocess_data(X, Y, one_hot=True):\n",
    "    # Normalize pixel values to [0, 1]\n",
    "    X = X / 255.0\n",
    "    # Reshape for CNN\n",
    "    X = X.reshape(-1, 28, 28)\n",
    "    \n",
    "    if not one_hot:  # Perform one-hot encoding only if required\n",
    "        # Ensure Y is integer type if not one-hot encoded\n",
    "        Y = Y.astype(int)\n",
    "        # One-hot encode labels\n",
    "        num_classes = np.max(Y) + 1\n",
    "        Y_one_hot = np.zeros((Y.size, num_classes))\n",
    "        Y_one_hot[np.arange(Y.size), Y] = 1\n",
    "        return X, Y_one_hot\n",
    "    else:\n",
    "        # If Y is already one-hot encoded, return it as-is\n",
    "        return X, Y\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7123b5f-d383-4145-9520-b4ca526fc153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 3.72%\n",
      "Shape of Y_test: (7172,)\n",
      "Shape of Y_pred: (7172, 25)\n"
     ]
    }
   ],
   "source": [
    "# Main Script\n",
    "if __name__ == \"__main__\":\n",
    "    # Preprocess test data\n",
    "    X_test, Y_test = preprocess_data(X_test, Y_test, one_hot=True)  # Adjust `one_hot` flag as needed\n",
    "    Y_pred = cnn.forward(X_test)\n",
    "\n",
    "    # Check accuracy based on the format of Y_test\n",
    "    if Y_test.ndim == 2:  # One-hot encoded\n",
    "        accuracy = np.mean(np.argmax(Y_pred, axis=1) == np.argmax(Y_test, axis=1))\n",
    "    else:  # Not one-hot encoded\n",
    "        accuracy = np.mean(np.argmax(Y_pred, axis=1) == Y_test)\n",
    "\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    print(\"Shape of Y_test:\", Y_test.shape)\n",
    "    print(\"Shape of Y_pred:\", Y_pred.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164d8692-6016-4172-97f2-e5965cbc94ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
