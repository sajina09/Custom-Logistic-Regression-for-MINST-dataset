{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14be3ba9-093a-40c0-9cf2-66c190d44d15",
   "metadata": {},
   "source": [
    "## Custom SVM class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a606e1f3-9eb4-4851-bb07-c8f7f45e71c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique training classes: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n",
      "Unique testing classes: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n",
      "Training Accuracy: 76.39%\n",
      "Testing Accuracy: 63.43%\n",
      "Confusion Matrix:\n",
      "[[323   0   0   0   0   0   0   0   0   0   0   0   3   4   0   0   0   1\n",
      "    0   0   0   0   0   0]\n",
      " [  0 349   0   0   0   0   0   0   0  75   0   0   0   0   0   0   0   0\n",
      "    0   0   0   8   0   0]\n",
      " [  0   0 264   0   0  16   0   0   0   0   5   0   0  19   0   0   0   0\n",
      "    0   0   0   0   6   0]\n",
      " [  0   0   0 153   0   0   0   0   0  15   0   0  14   0   0   0   0   5\n",
      "    0   0   6   0  52   0]\n",
      " [  0   0   0   0 435   0   0   0   0   0   0   0   0   0   0   0   0  63\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0  20   0   0 201   0   0   0   0   1   0   0   5   0   0   0   0\n",
      "    0   0   0  20   0   0]\n",
      " [  0   0   0  20   0   0 203  18   0   0   0   0  21  14   0  19   0   0\n",
      "   52   0   0   0   1   0]\n",
      " [  0   0   0   0  16   0  45 348   0   0   0   0   0   0   0   0   0   0\n",
      "   22   0   0   0   5   0]\n",
      " [  3   0   0   0   0   0   0   0 190   0   0   0  21   0   0  12   0  21\n",
      "    0   0   0   0   0  41]\n",
      " [  0   0   0   9   0  21   0   0  24 159   0   0   0   0   0   0  87   3\n",
      "    0   0   0   8   0  20]\n",
      " [  0   0   0   0   0   0   0   0   0   0 209   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [ 24   0   0   0  47   0   0   0   0   0   0 135  45   0   0  39   0 104\n",
      "    0   0   0   0   0   0]\n",
      " [ 42   0   0   4  14   0   0   0   0   0   0  15 139  10   0  33   0   2\n",
      "   32   0   0   0   0   0]\n",
      " [  0   0  17   0  21  21   8   0   0   0   0   0   0 138   0  20   0   0\n",
      "    5   4   0   0  12   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 324  22   0   0\n",
      "    1   0   0   0   0   0]\n",
      " [  0   0   0   0   0   3   0   0   0   0   0  21   0   0   0 140   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  21   0   0   0   0   0  41  62\n",
      "    0  20   0   0   0   0]\n",
      " [  0   0   0   3  22   0   0   0  40   0   0  57  27   0   0   3   0  43\n",
      "    0  18   0  12   0  21]\n",
      " [  0   0   1   0   0   0   0   0  21   0  40   0   0   0  21   0   0   0\n",
      "  104  20   0   0  41   0]\n",
      " [  0  17   0  39   0   3   0   0   0  41   0   0   0   0   0   0  56   0\n",
      "    0  80  30   0   0   0]\n",
      " [  0  11   0   3   0  35   0   0   0  16   0   0   0   0  19   0  32   0\n",
      "    0  50 131  32   0  17]\n",
      " [  0  20   0   0   0   3   0   0   0  20   0   0   0   0   0   0  22   0\n",
      "    0  17   1 123   0   0]\n",
      " [  0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   8  24\n",
      "   20   0   0  48 166   0]\n",
      " [  0   0   0  17   0   0   0   0  17   0   9   0   0   0   0   0  63   9\n",
      "   42   0   2  22   0 151]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.98      0.89       331\n",
      "           1       0.88      0.81      0.84       432\n",
      "           2       0.87      0.85      0.86       310\n",
      "           3       0.62      0.62      0.62       245\n",
      "           4       0.78      0.87      0.83       498\n",
      "           5       0.66      0.81      0.73       247\n",
      "           6       0.79      0.58      0.67       348\n",
      "           7       0.95      0.80      0.87       436\n",
      "           8       0.65      0.66      0.66       288\n",
      "          10       0.49      0.48      0.48       331\n",
      "          11       0.73      1.00      0.85       209\n",
      "          12       0.59      0.34      0.43       394\n",
      "          13       0.51      0.48      0.50       291\n",
      "          14       0.73      0.56      0.63       246\n",
      "          15       0.89      0.93      0.91       347\n",
      "          16       0.49      0.85      0.62       164\n",
      "          17       0.13      0.28      0.18       144\n",
      "          18       0.13      0.17      0.15       246\n",
      "          19       0.37      0.42      0.40       248\n",
      "          20       0.38      0.30      0.34       266\n",
      "          21       0.77      0.38      0.51       346\n",
      "          22       0.45      0.60      0.51       206\n",
      "          23       0.59      0.62      0.60       267\n",
      "          24       0.60      0.45      0.52       332\n",
      "\n",
      "    accuracy                           0.63      7172\n",
      "   macro avg       0.62      0.62      0.61      7172\n",
      "weighted avg       0.66      0.63      0.64      7172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class CustomLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000, num_classes=None):\n",
    "        \"\"\"\n",
    "        Initialize the Logistic Regression model\n",
    "        \n",
    "        Parameters:\n",
    "        - learning_rate: step size for gradient descent\n",
    "        - num_iterations: number of training iterations\n",
    "        - num_classes: number of unique classes in the dataset\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.num_classes = num_classes\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.class_mapping = None  # To handle zero-based indexing\n",
    "\n",
    "    def _softmax(self, z):\n",
    "        \"\"\"\n",
    "        Softmax activation function for multiclass classification\n",
    "        Prevents numerical instability by subtracting max value\n",
    "        \n",
    "        Parameters:\n",
    "        - z: input array of logits\n",
    "        \n",
    "        Returns:\n",
    "        - Softmax probabilities\n",
    "        \"\"\"\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def _one_hot_encode(self, y):\n",
    "        \"\"\"\n",
    "        Convert labels to one-hot encoded format\n",
    "        \n",
    "        Parameters:\n",
    "        - y: original labels\n",
    "        \n",
    "        Returns:\n",
    "        - One-hot encoded labels\n",
    "        \"\"\"\n",
    "        # Remap labels to zero-based index if needed\n",
    "        if self.class_mapping is None:\n",
    "            unique_classes = np.unique(y)\n",
    "            self.class_mapping = {orig: idx for idx, orig in enumerate(unique_classes)}\n",
    "            self.reverse_mapping = {idx: orig for orig, idx in self.class_mapping.items()}\n",
    "        \n",
    "        # Map original labels to zero-based index\n",
    "        y_mapped = np.array([self.class_mapping[label] for label in y])\n",
    "        \n",
    "        # Create one-hot encoding\n",
    "        one_hot = np.zeros((y.shape[0], self.num_classes))\n",
    "        one_hot[np.arange(y.shape[0]), y_mapped] = 1\n",
    "        return one_hot\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the logistic regression model\n",
    "        \n",
    "        Parameters:\n",
    "        - X: input features (num_samples, num_features)\n",
    "        - y: target labels\n",
    "        \"\"\"\n",
    "        # Determine number of classes if not specified\n",
    "        unique_classes = np.unique(y)\n",
    "        self.num_classes = len(unique_classes)\n",
    "        \n",
    "        # Initialize weights and bias\n",
    "        num_features = X.shape[1]\n",
    "        self.weights = np.zeros((num_features, self.num_classes))\n",
    "        self.bias = np.zeros((1, self.num_classes))\n",
    "        \n",
    "        # One-hot encode labels\n",
    "        Y_one_hot = self._one_hot_encode(y)\n",
    "        \n",
    "        # Gradient descent\n",
    "        for _ in range(self.num_iterations):\n",
    "            # Forward pass\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self._softmax(linear_model)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw = (1/X.shape[0]) * np.dot(X.T, (y_predicted - Y_one_hot))\n",
    "            db = (1/X.shape[0]) * np.sum(y_predicted - Y_one_hot, axis=0, keepdims=True)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on input data\n",
    "        \n",
    "        Parameters:\n",
    "        - X: input features\n",
    "        \n",
    "        Returns:\n",
    "        - Predicted class labels (original class labels)\n",
    "        \"\"\"\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self._softmax(linear_model)\n",
    "        \n",
    "        # Get indices of max probabilities\n",
    "        predicted_indices = np.argmax(y_predicted, axis=1)\n",
    "        \n",
    "        # Map back to original class labels\n",
    "        return np.array([self.reverse_mapping[idx] for idx in predicted_indices])\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities\n",
    "        \n",
    "        Parameters:\n",
    "        - X: input features\n",
    "        \n",
    "        Returns:\n",
    "        - Predicted class probabilities\n",
    "        \"\"\"\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        return self._softmax(linear_model)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute model accuracy\n",
    "        \n",
    "        Parameters:\n",
    "        - X: input features\n",
    "        - y: true labels\n",
    "        \n",
    "        Returns:\n",
    "        - Accuracy score\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "\n",
    "def preprocess_sign_mnist(train_data, test_data):\n",
    "    \"\"\"\n",
    "    Preprocess Sign MNIST dataset\n",
    "    \n",
    "    Parameters:\n",
    "    - train_data: Training dataframe\n",
    "    - test_data: Testing dataframe\n",
    "    \n",
    "    Returns:\n",
    "    - Preprocessed X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    # Separate features and labels\n",
    "    X_train = train_data.drop('label', axis=1).values\n",
    "    y_train = train_data['label'].values\n",
    "    \n",
    "    X_test = test_data.drop('label', axis=1).values\n",
    "    y_test = test_data['label'].values\n",
    "    \n",
    "    # Normalize pixel values to [0, 1]\n",
    "    X_train = X_train.astype('float32') / 255.0\n",
    "    X_test = X_test.astype('float32') / 255.0\n",
    "    \n",
    "    print(\"Unique training classes:\", np.unique(y_train))\n",
    "    print(\"Unique testing classes:\", np.unique(y_test))\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_logistic_regression(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train custom Logistic Regression on Sign MNIST\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Training features\n",
    "    - X_test: Testing features\n",
    "    - y_train: Training labels\n",
    "    - y_test: Testing labels\n",
    "    \n",
    "    Returns:\n",
    "    - Trained model\n",
    "    - Training and test accuracies\n",
    "    \"\"\"\n",
    "    # Create and train the model\n",
    "    clf = CustomLogisticRegression(\n",
    "        learning_rate=0.1,  # You can tune this\n",
    "        num_iterations=1000,  # You can increase for better convergence\n",
    "        num_classes=len(np.unique(y_train))\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Compute accuracies\n",
    "    train_accuracy = clf.accuracy(X_train, y_train)\n",
    "    test_accuracy = clf.accuracy(X_test, y_test)\n",
    "    \n",
    "    return clf, train_accuracy, test_accuracy\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    # Load the training dataset\n",
    "    train_csv_path = './archive/sign_mnist_train.csv'\n",
    "    train_data = pd.read_csv(train_csv_path)\n",
    "    \n",
    "    # Load the testing dataset\n",
    "    test_csv_path = './archive/sign_mnist_test.csv'\n",
    "    test_data = pd.read_csv(test_csv_path)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    X_train, X_test, y_train, y_test = preprocess_sign_mnist(train_data, test_data)\n",
    "    \n",
    "    # Train the model\n",
    "    model, train_acc, test_acc = train_logistic_regression(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Training Accuracy: {train_acc * 100:.2f}%\")\n",
    "    print(f\"Testing Accuracy: {test_acc * 100:.2f}%\")\n",
    "    \n",
    "    return model, X_test, y_test\n",
    "\n",
    "# Optional: Confusion Matrix and Classification Report\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Generate detailed model evaluation\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained logistic regression model\n",
    "    - X_test: Test features\n",
    "    - y_test: Test labels\n",
    "    \"\"\"\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    model, X_test, y_test = main()\n",
    "    evaluate_model(model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
