{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14be3ba9-093a-40c0-9cf2-66c190d44d15",
   "metadata": {},
   "source": [
    "## Custom SVM class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a606e1f3-9eb4-4851-bb07-c8f7f45e71c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class CustomLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000, num_classes=None):\n",
    "        \"\"\"\n",
    "        Initialize the Logistic Regression model\n",
    "        \n",
    "        Parameters:\n",
    "        - learning_rate: step size for gradient descent\n",
    "        - num_iterations: number of training iterations\n",
    "        - num_classes: number of unique classes in the dataset\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.num_classes = num_classes\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.class_mapping = None  # To handle zero-based indexing\n",
    "\n",
    "    def _softmax(self, z):\n",
    "        \"\"\"\n",
    "        Softmax activation function for multiclass classification\n",
    "        Prevents numerical instability by subtracting max value\n",
    "        \n",
    "        Parameters:\n",
    "        - z: input array of logits\n",
    "        \n",
    "        Returns:\n",
    "        - Softmax probabilities\n",
    "        \"\"\"\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def _one_hot_encode(self, y):\n",
    "        \"\"\"\n",
    "        Convert labels to one-hot encoded format\n",
    "        \n",
    "        Parameters:\n",
    "        - y: original labels\n",
    "        \n",
    "        Returns:\n",
    "        - One-hot encoded labels\n",
    "        \"\"\"\n",
    "        # Remap labels to zero-based index if needed\n",
    "        if self.class_mapping is None:\n",
    "            unique_classes = np.unique(y)\n",
    "            self.class_mapping = {orig: idx for idx, orig in enumerate(unique_classes)}\n",
    "            self.reverse_mapping = {idx: orig for orig, idx in self.class_mapping.items()}\n",
    "        \n",
    "        # Map original labels to zero-based index\n",
    "        y_mapped = np.array([self.class_mapping[label] for label in y])\n",
    "        \n",
    "        # Create one-hot encoding\n",
    "        one_hot = np.zeros((y.shape[0], self.num_classes))\n",
    "        one_hot[np.arange(y.shape[0]), y_mapped] = 1\n",
    "        return one_hot\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the logistic regression model\n",
    "        \n",
    "        Parameters:\n",
    "        - X: input features (num_samples, num_features)\n",
    "        - y: target labels\n",
    "        \"\"\"\n",
    "        # Determine number of classes if not specified\n",
    "        unique_classes = np.unique(y)\n",
    "        self.num_classes = len(unique_classes)\n",
    "        \n",
    "        # Initialize weights and bias\n",
    "        num_features = X.shape[1]\n",
    "        self.weights = np.zeros((num_features, self.num_classes))\n",
    "        self.bias = np.zeros((1, self.num_classes))\n",
    "        \n",
    "        # One-hot encode labels\n",
    "        Y_one_hot = self._one_hot_encode(y)\n",
    "        \n",
    "        # Gradient descent\n",
    "        for _ in range(self.num_iterations):\n",
    "            # Forward pass\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self._softmax(linear_model)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw = (1/X.shape[0]) * np.dot(X.T, (y_predicted - Y_one_hot))\n",
    "            db = (1/X.shape[0]) * np.sum(y_predicted - Y_one_hot, axis=0, keepdims=True)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on input data\n",
    "        \n",
    "        Parameters:\n",
    "        - X: input features\n",
    "        \n",
    "        Returns:\n",
    "        - Predicted class labels (original class labels)\n",
    "        \"\"\"\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self._softmax(linear_model)\n",
    "        \n",
    "        # Get indices of max probabilities\n",
    "        predicted_indices = np.argmax(y_predicted, axis=1)\n",
    "        \n",
    "        # Map back to original class labels\n",
    "        return np.array([self.reverse_mapping[idx] for idx in predicted_indices])\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities\n",
    "        \n",
    "        Parameters:\n",
    "        - X: input features\n",
    "        \n",
    "        Returns:\n",
    "        - Predicted class probabilities\n",
    "        \"\"\"\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        return self._softmax(linear_model)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute model accuracy\n",
    "        \n",
    "        Parameters:\n",
    "        - X: input features\n",
    "        - y: true labels\n",
    "        \n",
    "        Returns:\n",
    "        - Accuracy score\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "\n",
    "def preprocess_sign_mnist(train_data, test_data):\n",
    "    \"\"\"\n",
    "    Preprocess Sign MNIST dataset\n",
    "    \n",
    "    Parameters:\n",
    "    - train_data: Training dataframe\n",
    "    - test_data: Testing dataframe\n",
    "    \n",
    "    Returns:\n",
    "    - Preprocessed X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    # Separate features and labels\n",
    "    X_train = train_data.drop('label', axis=1).values\n",
    "    y_train = train_data['label'].values\n",
    "    \n",
    "    X_test = test_data.drop('label', axis=1).values\n",
    "    y_test = test_data['label'].values\n",
    "    \n",
    "    # Normalize pixel values to [0, 1]\n",
    "    X_train = X_train.astype('float32') / 255.0\n",
    "    X_test = X_test.astype('float32') / 255.0\n",
    "    \n",
    "    print(\"Unique training classes:\", np.unique(y_train))\n",
    "    print(\"Unique testing classes:\", np.unique(y_test))\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_logistic_regression(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train custom Logistic Regression on Sign MNIST\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Training features\n",
    "    - X_test: Testing features\n",
    "    - y_train: Training labels\n",
    "    - y_test: Testing labels\n",
    "    \n",
    "    Returns:\n",
    "    - Trained model\n",
    "    - Training and test accuracies\n",
    "    \"\"\"\n",
    "    # Create and train the model\n",
    "    clf = CustomLogisticRegression(\n",
    "        learning_rate=0.1,  # You can tune this\n",
    "        num_iterations=1000,  # You can increase for better convergence\n",
    "        num_classes=len(np.unique(y_train))\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Compute accuracies\n",
    "    train_accuracy = clf.accuracy(X_train, y_train)\n",
    "    test_accuracy = clf.accuracy(X_test, y_test)\n",
    "    \n",
    "    return clf, train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3805fd7-9fae-4c93-a1fb-e496ef9dc1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path = './archive/sign_mnist_train.csv'\n",
    "train_data = pd.read_csv(train_csv_path)\n",
    "    \n",
    "    # Load the testing dataset\n",
    "test_csv_path = './archive/sign_mnist_test.csv'\n",
    "test_data = pd.read_csv(test_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88427862-a4d0-4eb8-b78e-a8aa74fde753",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Preprocess the data\n",
    "X_train, X_test, y_train, y_test = preprocess_sign_mnist(train_data, test_data)\n",
    "    \n",
    "    # Train the model\n",
    "model, train_acc, test_acc = train_logistic_regression(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc * 100:.2f}%\")\n",
    "print(f\"Testing Accuracy: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e22272c-a7be-4e45-84c8-1361b37a2531",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "771a96f7-2727-4b4f-93cb-ca8cb37f0b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_confusion_matrix(y_true, y_pred, num_classes, class_names=None):\n",
    "    \"\"\"\n",
    "    Compute the confusion matrix manually and plot it with a stylish design.\n",
    "\n",
    "    Parameters:\n",
    "        y_true: Array of true labels\n",
    "        y_pred: Array of predicted labels\n",
    "        num_classes: Number of unique classes\n",
    "        class_names: List of class names corresponding to the classes (optional)\n",
    "\n",
    "    Returns:\n",
    "        Confusion matrix as a 2D numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize confusion matrix\n",
    "    matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        matrix[true][pred] += 1  # Increment the cell corresponding to (true, predicted)\n",
    "\n",
    "    # Plotting the styled confusion matrix\n",
    "    plt.figure(figsize=(12,10))\n",
    "    sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                xticklabels=class_names if class_names else range(num_classes),\n",
    "                yticklabels=class_names if class_names else range(num_classes),\n",
    "                linewidths=0.5, linecolor='black')\n",
    "\n",
    "    plt.title('Confusion Matrix', fontsize=14)\n",
    "    plt.xlabel('Predicted Labels', fontsize=12)\n",
    "    plt.ylabel('True Labels', fontsize=12)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12, rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def custom_classification_report(y_true, y_pred, num_classes, class_names=None):\n",
    "    \"\"\"\n",
    "    Compute and display the classification report in a styled table format.\n",
    "\n",
    "    Parameters:\n",
    "        y_true: Array of true labels\n",
    "        y_pred: Array of predicted labels\n",
    "        num_classes: Number of unique classes\n",
    "        class_names: List of class names (optional)\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing precision, recall, F1-score, and support.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = custom_confusion_matrix(y_true, y_pred, num_classes)\n",
    "\n",
    "    # Initialize report dictionary\n",
    "    report = {}\n",
    "    for cls in range(num_classes):\n",
    "        true_positive = cm[cls, cls]\n",
    "        false_positive = sum(cm[:, cls]) - true_positive\n",
    "        false_negative = sum(cm[cls, :]) - true_positive\n",
    "\n",
    "        # Handle cases with no positive or negative samples for this class\n",
    "        precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0.0\n",
    "        recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0.0\n",
    "        f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        support = sum(y_true == cls)\n",
    "\n",
    "        report[cls] = {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1-score\": f1_score,\n",
    "            \"support\": support\n",
    "        }\n",
    "\n",
    "    # Convert the report dictionary to a DataFrame\n",
    "    class_names = class_names if class_names else [f\"Class {i}\" for i in range(num_classes)]\n",
    "    df_report = pd.DataFrame.from_dict(report, orient='index')\n",
    "    df_report.index = class_names\n",
    "\n",
    "    # Add average metrics\n",
    "    macro_avg = df_report[[\"precision\", \"recall\", \"f1-score\"]].mean()\n",
    "    weighted_avg = df_report[[\"precision\", \"recall\", \"f1-score\"]].multiply(df_report[\"support\"], axis=0).sum() / df_report[\"support\"].sum()\n",
    "\n",
    "    df_report.loc[\"macro avg\"] = macro_avg\n",
    "    df_report.loc[\"weighted avg\"] = weighted_avg\n",
    "\n",
    "    print(\"Accuracy: \", sum(y_t == y_p for y_t, y_p in zip(y_true, y_pred)) / len(y_true))\n",
    "\n",
    "    return df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b555d2-c46a-4a32-b2d1-254242e50c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# cm = custom_classification_report(y_test, y_pred)\n",
    "\n",
    "# Number of unique classes\n",
    "num_classes = len(np.unique(y_test)) + 1\n",
    "\n",
    "# Optional: Define class names (e.g., 0-23 for Sign Language MNIST)\n",
    "class_names = [str(i) for i in range(num_classes)]\n",
    "\n",
    "svm_report = custom_classification_report(y_test, y_pred,num_classes=num_classes, class_names=class_names)\n",
    "print(svm_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87becf5-da24-459a-8da4-f67f2e5bbf0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
