{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "864c9846-79cb-47c6-9921-be7a84b74de5",
   "metadata": {
    "id": "864c9846-79cb-47c6-9921-be7a84b74de5"
   },
   "source": [
    "## We are using MINST sign languge dataset from Kaggel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4745c67-052d-47fd-860b-10708536c99b",
   "metadata": {
    "id": "a4745c67-052d-47fd-860b-10708536c99b"
   },
   "source": [
    "The original MNIST(Modified National Institute of Standards and Technology) image dataset of handwritten digits is a popular benchmark for image-based machine learning methods. However it doesn't work for 2 of the english alphabets J and Z because it requires gesture motion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f01770-938f-47ea-8de2-b90801ced7dc",
   "metadata": {
    "id": "f4f01770-938f-47ea-8de2-b90801ced7dc"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D5S_RPlC0bfE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D5S_RPlC0bfE",
    "outputId": "a3126946-9bb9-45bb-eb95-94780031a684"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define paths to the dataset\n",
    "train_csv_path = '/content/drive/My Drive/Colab Notebooks/archive/sign_mnist_train.csv'\n",
    "test_csv_path = '/content/drive/My Drive/Colab Notebooks/archive/sign_mnist_test.csv'\n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv(train_csv_path)\n",
    "test_data = pd.read_csv(test_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288b4e71-95e6-4830-9e3e-da9a37d19263",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "288b4e71-95e6-4830-9e3e-da9a37d19263",
    "outputId": "12a04dbc-40f7-45d9-cbf2-434cd2be14da",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking unique value in dataset\n",
    "\n",
    "labels=train_data.label.unique()\n",
    "np.sort(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2440083-cdfc-423d-8575-3cda0744d606",
   "metadata": {
    "id": "d2440083-cdfc-423d-8575-3cda0744d606"
   },
   "outputs": [],
   "source": [
    "# Converting the pandas Dataframe into Numpy Arrays\n",
    "\n",
    "inputs_array_train = train_data.iloc[:, 1:].to_numpy()\n",
    "targets_array_train = train_data['label'].to_numpy()\n",
    "inputs_array_test = test_data.iloc[:, 1:].to_numpy()\n",
    "targets_array_test = test_data['label'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5JnxAMS2QF16",
   "metadata": {
    "id": "5JnxAMS2QF16"
   },
   "outputs": [],
   "source": [
    "class CustomLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000, num_classes=None):\n",
    "        \"\"\"\n",
    "        Initialize the Logistic Regression model\n",
    "\n",
    "        Parameters:\n",
    "        - learning_rate: step size for gradient descent\n",
    "        - num_iterations: number of training iterations\n",
    "        - num_classes: number of unique classes in the dataset\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.num_classes = num_classes\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.class_mapping = None  # To handle zero-based indexing\n",
    "\n",
    "    def _softmax(self, z):\n",
    "        \"\"\"\n",
    "        Softmax activation function for multiclass classification\n",
    "        Prevents numerical instability by subtracting max value\n",
    "\n",
    "        Parameters:\n",
    "        - z: input array of logits\n",
    "\n",
    "        Returns:\n",
    "        - Softmax probabilities\n",
    "        \"\"\"\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def _one_hot_encode(self, y):\n",
    "        \"\"\"\n",
    "        Convert labels to one-hot encoded format\n",
    "\n",
    "        Parameters:\n",
    "        - y: original labels\n",
    "\n",
    "        Returns:\n",
    "        - One-hot encoded labels\n",
    "        \"\"\"\n",
    "        if self.class_mapping is None:\n",
    "            unique_classes = np.unique(y)\n",
    "            self.class_mapping = {orig: idx for idx, orig in enumerate(unique_classes)}\n",
    "            self.reverse_mapping = {idx: orig for orig, idx in self.class_mapping.items()}\n",
    "\n",
    "        y_mapped = np.array([self.class_mapping[label] for label in y])\n",
    "        one_hot = np.zeros((y.shape[0], self.num_classes))\n",
    "        one_hot[np.arange(y.shape[0]), y_mapped] = 1\n",
    "        return one_hot\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the logistic regression model\n",
    "\n",
    "        Parameters:\n",
    "        - X: input features (num_samples, num_features)\n",
    "        - y: target labels\n",
    "        \"\"\"\n",
    "        self.num_classes = len(np.unique(y))  # Automatically detect number of classes\n",
    "        num_features = X.shape[1]\n",
    "        self.weights = np.zeros((num_features, self.num_classes))\n",
    "        self.bias = np.zeros((1, self.num_classes))\n",
    "        Y_one_hot = self._one_hot_encode(y)\n",
    "\n",
    "        for i in range(self.num_iterations):\n",
    "            # Forward pass\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self._softmax(linear_model)\n",
    "\n",
    "            # Compute gradients\n",
    "            dw = (1 / X.shape[0]) * np.dot(X.T, (y_predicted - Y_one_hot))\n",
    "            db = (1 / X.shape[0]) * np.sum(y_predicted - Y_one_hot, axis=0, keepdims=True)\n",
    "\n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "            # Print progress every 100 iterations\n",
    "            if i % 100 == 0:\n",
    "                loss = -np.sum(Y_one_hot * np.log(y_predicted + 1e-9)) / X.shape[0]\n",
    "                print(f\"Iteration {i}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on input data\n",
    "        \"\"\"\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self._softmax(linear_model)\n",
    "        predicted_indices = np.argmax(y_predicted, axis=1)\n",
    "        return np.array([self.reverse_mapping[idx] for idx in predicted_indices])\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute accuracy\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "\n",
    "\n",
    "def preprocess_sign_mnist(train_data, test_data):\n",
    "    \"\"\"\n",
    "    Preprocess Sign MNIST dataset\n",
    "    \"\"\"\n",
    "    X_train = train_data.drop('label', axis=1).values\n",
    "    y_train = train_data['label'].values\n",
    "    X_test = test_data.drop('label', axis=1).values\n",
    "    y_test = test_data['label'].values\n",
    "\n",
    "    X_train = X_train.astype('float32') / 255.0\n",
    "    X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "    print(\"Data preprocessing complete.\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def train_logistic_regression(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train custom Logistic Regression on Sign MNIST\n",
    "    \"\"\"\n",
    "    clf = CustomLogisticRegression(learning_rate=0.1, num_iterations=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    train_accuracy = clf.accuracy(X_train, y_train)\n",
    "    test_accuracy = clf.accuracy(X_test, y_test)\n",
    "\n",
    "    print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "    print(f\"Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "    return clf\n",
    "\n",
    "\n",
    "# Example Workflow\n",
    "# Assuming `train_data` and `test_data` are pandas DataFrames with \"label\" column\n",
    "# X_train, X_test, y_train, y_test = preprocess_sign_mnist(train_data, test_data)\n",
    "# clf = train_logistic_regression(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nYTxuBg_QFK1",
   "metadata": {
    "id": "nYTxuBg_QFK1"
   },
   "outputs": [],
   "source": [
    "class CustomPCA:\n",
    "    def __init__(self, n_components=None, variance_threshold=None):\n",
    "        \"\"\"\n",
    "        Initialize CustomPCA.\n",
    "        :param n_components: Number of principal components to retain (optional).\n",
    "        :param variance_threshold: Fraction of variance to retain (optional).\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.variance_threshold = variance_threshold\n",
    "        self.components = None  # Principal components\n",
    "        self.mean = None        # Mean of each feature\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        Fit the PCA model and transform the data.\n",
    "        :param X: Feature matrix (n_samples, n_features)\n",
    "        :return: Transformed data with reduced dimensions\n",
    "        \"\"\"\n",
    "        # Center the data by subtracting the mean\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        X_centered = X - self.mean\n",
    "\n",
    "        # Compute the covariance matrix\n",
    "        covariance_matrix = np.cov(X_centered, rowvar=False)\n",
    "\n",
    "        # Compute eigenvalues and eigenvectors\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "\n",
    "        # Sort eigenvalues and eigenvectors in descending order\n",
    "        sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvalues = eigenvalues[sorted_indices]\n",
    "        eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "        # Select the number of components based on n_components or variance threshold\n",
    "        if self.n_components is not None:\n",
    "            eigenvectors = eigenvectors[:, :self.n_components]\n",
    "        elif self.variance_threshold is not None:\n",
    "            total_variance = np.sum(eigenvalues)\n",
    "            variance_retained = 0\n",
    "            num_components = 0\n",
    "            for eigenvalue in eigenvalues:\n",
    "                variance_retained += eigenvalue\n",
    "                num_components += 1\n",
    "                if variance_retained / total_variance >= self.variance_threshold:\n",
    "                    break\n",
    "            eigenvectors = eigenvectors[:, :num_components]\n",
    "\n",
    "        self.components = eigenvectors\n",
    "\n",
    "        # Project data onto the selected principal components\n",
    "        return np.dot(X_centered, self.components)\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform data using the fitted PCA model.\n",
    "        :param X: Feature matrix (n_samples, n_features)\n",
    "        :return: Transformed data\n",
    "        \"\"\"\n",
    "        X_centered = X - self.mean\n",
    "        return np.dot(X_centered, self.components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lnBO9btSQHXc",
   "metadata": {
    "id": "lnBO9btSQHXc"
   },
   "outputs": [],
   "source": [
    "def custom_confusion_matrix(y_true, y_pred, num_classes, class_names=None):\n",
    "    \"\"\"\n",
    "    Compute the confusion matrix manually and plot it with a stylish design.\n",
    "\n",
    "    Parameters:\n",
    "        y_true: Array of true labels\n",
    "        y_pred: Array of predicted labels\n",
    "        num_classes: Number of unique classes\n",
    "        class_names: List of class names corresponding to the classes (optional)\n",
    "\n",
    "    Returns:\n",
    "        Confusion matrix as a 2D numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize confusion matrix\n",
    "    matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        matrix[true][pred] += 1  # Increment the cell corresponding to (true, predicted)\n",
    "\n",
    "    # Plotting the styled confusion matrix\n",
    "    plt.figure(figsize=(12,10))\n",
    "    sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                xticklabels=class_names if class_names else range(num_classes),\n",
    "                yticklabels=class_names if class_names else range(num_classes),\n",
    "                linewidths=0.5, linecolor='black')\n",
    "\n",
    "    plt.title('Confusion Matrix', fontsize=14)\n",
    "    plt.xlabel('Predicted Labels', fontsize=12)\n",
    "    plt.ylabel('True Labels', fontsize=12)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12, rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def custom_classification_report(y_true, y_pred, num_classes, class_names=None):\n",
    "    \"\"\"\n",
    "    Compute and display the classification report in a styled table format.\n",
    "\n",
    "    Parameters:\n",
    "        y_true: Array of true labels\n",
    "        y_pred: Array of predicted labels\n",
    "        num_classes: Number of unique classes\n",
    "        class_names: List of class names (optional)\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing precision, recall, F1-score, and support.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = custom_confusion_matrix(y_true, y_pred, num_classes)\n",
    "\n",
    "    # Initialize report dictionary\n",
    "    report = {}\n",
    "    for cls in range(num_classes):\n",
    "        true_positive = cm[cls, cls]\n",
    "        false_positive = sum(cm[:, cls]) - true_positive\n",
    "        false_negative = sum(cm[cls, :]) - true_positive\n",
    "\n",
    "        # Handle cases with no positive or negative samples for this class\n",
    "        precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0.0\n",
    "        recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0.0\n",
    "        f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        support = sum(y_true == cls)\n",
    "\n",
    "        report[cls] = {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1-score\": f1_score,\n",
    "            \"support\": support\n",
    "        }\n",
    "\n",
    "    # Convert the report dictionary to a DataFrame\n",
    "    class_names = class_names if class_names else [f\"Class {i}\" for i in range(num_classes)]\n",
    "    df_report = pd.DataFrame.from_dict(report, orient='index')\n",
    "    df_report.index = class_names\n",
    "\n",
    "    # Add average metrics\n",
    "    macro_avg = df_report[[\"precision\", \"recall\", \"f1-score\"]].mean()\n",
    "    weighted_avg = df_report[[\"precision\", \"recall\", \"f1-score\"]].multiply(df_report[\"support\"], axis=0).sum() / df_report[\"support\"].sum()\n",
    "\n",
    "    df_report.loc[\"macro avg\"] = macro_avg\n",
    "    df_report.loc[\"weighted avg\"] = weighted_avg\n",
    "\n",
    "    return df_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R854ZwrAQkn_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R854ZwrAQkn_",
    "outputId": "e6622740-0a72-4fe1-99f9-1778693dc508"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 3.1781\n",
      "Iteration 100, Loss: 2.3069\n",
      "Iteration 200, Loss: 1.9253\n",
      "Iteration 300, Loss: 1.7007\n",
      "Iteration 400, Loss: 1.5470\n",
      "Iteration 500, Loss: 1.4323\n",
      "Iteration 600, Loss: 1.3419\n",
      "Iteration 700, Loss: 1.2679\n",
      "Iteration 800, Loss: 1.2056\n",
      "Iteration 900, Loss: 1.1520\n",
      "Training Accuracy: 76.39%\n",
      "Testing Accuracy: 63.43%\n"
     ]
    }
   ],
   "source": [
    "# Create and train the model\n",
    "clf = CustomLogisticRegression(\n",
    "    learning_rate=0.1,\n",
    "    num_iterations=1000,\n",
    "    num_classes=len(np.unique(y_train))\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Compute accuracies\n",
    "train_accuracy = clf.accuracy(X_train, y_train)\n",
    "test_accuracy = clf.accuracy(X_test, y_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Testing Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KZI-NRT-gaeA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KZI-NRT-gaeA",
    "outputId": "cecbdd7d-d992-48dc-deec-86c7e94309ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (27455, 784)\n",
      "Shape of X_test: (7172, 784)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1Fb9yb9lfJFK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "1Fb9yb9lfJFK",
    "outputId": "f4e0f466-d379-416a-f356-29ce5fd381f1"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (27455,784) and (113,) not aligned: 784 (dim 1) != 113 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-0c75bfcd5533>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Step 1: Predict labels for training and testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_train_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Step 2: Number of classes and class names (optional)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-89fe7ae4ef47>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mlinear_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (27455,784) and (113,) not aligned: 784 (dim 1) != 113 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Step 1: Predict labels for training and testing data\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "# Step 2: Number of classes and class names (optional)\n",
    "num_classes = len(np.unique(y_train))\n",
    "class_names = [f\"Class {i}\" for i in range(num_classes)]  # You can define specific class names if needed\n",
    "\n",
    "# Step 3: Generate and display classification report for training data\n",
    "print(\"Classification Report for Training Data:\")\n",
    "train_report = custom_classification_report(y_train, y_train_pred, num_classes, class_names)\n",
    "print(train_report)\n",
    "\n",
    "# Step 4: Generate and display classification report for testing data\n",
    "print(\"\\nClassification Report for Testing Data:\")\n",
    "test_report = custom_classification_report(y_test, y_test_pred, num_classes, class_names)\n",
    "print(test_report)\n",
    "\n",
    "# Step 5: Optionally save or display the confusion matrix\n",
    "print(\"\\nConfusion Matrix for Testing Data:\")\n",
    "custom_confusion_matrix(y_test, y_test_pred, num_classes, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xkw9K8yfQyZs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xkw9K8yfQyZs",
    "outputId": "1cfa81a4-2da9-4048-b7b4-eb933bff7d1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 73.62%\n",
      "Testing Accuracy: 62.65%\n"
     ]
    }
   ],
   "source": [
    "# Apply Custom PCA to reduce dimensionality\n",
    "custom_pca = CustomPCA(variance_threshold=0.95)  # Retain 95% variance\n",
    "X_train_pca = custom_pca.fit_transform(X_train)\n",
    "X_test_pca = custom_pca.transform(X_test)\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(X_train_pca, y_train)\n",
    "train_accuracy = clf.accuracy(X_train_pca, y_train)  # Training data and labels\n",
    "test_accuracy = clf.accuracy(X_test_pca, y_test)    # Testing data and labels\n",
    "\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Testing Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7z_Wht3YbG1F",
   "metadata": {
    "id": "7z_Wht3YbG1F"
   },
   "source": [
    "### Shuffling input data to train to see if the accuracy changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HTs-I18iX9j9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HTs-I18iX9j9",
    "outputId": "64a854cd-4367-4a83-9790-19f16bb69c26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 73.62%\n",
      "Testing Accuracy: 62.65%\n"
     ]
    }
   ],
   "source": [
    "# Function to shuffle data and labels together\n",
    "def shuffle_data(X, y):\n",
    "    # Generate a permutation of indices\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    # Shuffle data and labels\n",
    "    return X[indices], y[indices]\n",
    "\n",
    "# Shuffle the training data\n",
    "X_train_shuffled, y_train_shuffled = shuffle_data(X_train, y_train)\n",
    "\n",
    "# If you are using PCA, apply it on the shuffled data\n",
    "custom_pca = CustomPCA(variance_threshold=0.95)  # Retain 95% variance\n",
    "X_train_pca_shuffled = custom_pca.fit_transform(X_train_shuffled)\n",
    "X_test_pca = custom_pca.transform(X_test)\n",
    "\n",
    "# Train the model on the shuffled data\n",
    "clf.fit(X_train_pca_shuffled, y_train_shuffled)\n",
    "\n",
    "# Compute accuracies\n",
    "train_accuracy = clf.accuracy(X_train_pca_shuffled, y_train_shuffled)\n",
    "test_accuracy = clf.accuracy(X_test_pca, y_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Testing Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wootJ8OCb4i0",
   "metadata": {
    "id": "wootJ8OCb4i0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
