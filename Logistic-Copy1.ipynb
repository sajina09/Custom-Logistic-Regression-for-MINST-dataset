{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd874c62-8f76-4f9c-9fb8-5adb59290cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ff92119-690f-4a60-bcbe-b7c8842988e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training dataset\n",
    "train_csv_path = './archive/sign_mnist_train.csv'\n",
    "train_data = pd.read_csv(train_csv_path)\n",
    "\n",
    "# Load the testing dataset\n",
    "test_csv_path = './archive/sign_mnist_test.csv'\n",
    "test_data = pd.read_csv(test_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a75e225-6a12-4877-931b-ac4680d952ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique training classes: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n",
      "Unique testing classes: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n",
      "Training Accuracy: 76.39%\n",
      "Testing Accuracy: 63.43%\n",
      "Confusion Matrix:\n",
      "[[323   0   0   0   0   0   0   0   0   0   0   0   3   4   0   0   0   1\n",
      "    0   0   0   0   0   0]\n",
      " [  0 349   0   0   0   0   0   0   0  75   0   0   0   0   0   0   0   0\n",
      "    0   0   0   8   0   0]\n",
      " [  0   0 264   0   0  16   0   0   0   0   5   0   0  19   0   0   0   0\n",
      "    0   0   0   0   6   0]\n",
      " [  0   0   0 153   0   0   0   0   0  15   0   0  14   0   0   0   0   5\n",
      "    0   0   6   0  52   0]\n",
      " [  0   0   0   0 435   0   0   0   0   0   0   0   0   0   0   0   0  63\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0  20   0   0 201   0   0   0   0   1   0   0   5   0   0   0   0\n",
      "    0   0   0  20   0   0]\n",
      " [  0   0   0  20   0   0 203  18   0   0   0   0  21  14   0  19   0   0\n",
      "   52   0   0   0   1   0]\n",
      " [  0   0   0   0  16   0  45 348   0   0   0   0   0   0   0   0   0   0\n",
      "   22   0   0   0   5   0]\n",
      " [  3   0   0   0   0   0   0   0 190   0   0   0  21   0   0  12   0  21\n",
      "    0   0   0   0   0  41]\n",
      " [  0   0   0   9   0  21   0   0  24 159   0   0   0   0   0   0  87   3\n",
      "    0   0   0   8   0  20]\n",
      " [  0   0   0   0   0   0   0   0   0   0 209   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [ 24   0   0   0  47   0   0   0   0   0   0 135  45   0   0  39   0 104\n",
      "    0   0   0   0   0   0]\n",
      " [ 42   0   0   4  14   0   0   0   0   0   0  15 139  10   0  33   0   2\n",
      "   32   0   0   0   0   0]\n",
      " [  0   0  17   0  21  21   8   0   0   0   0   0   0 138   0  20   0   0\n",
      "    5   4   0   0  12   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 324  22   0   0\n",
      "    1   0   0   0   0   0]\n",
      " [  0   0   0   0   0   3   0   0   0   0   0  21   0   0   0 140   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  21   0   0   0   0   0  41  62\n",
      "    0  20   0   0   0   0]\n",
      " [  0   0   0   3  22   0   0   0  40   0   0  57  27   0   0   3   0  43\n",
      "    0  18   0  12   0  21]\n",
      " [  0   0   1   0   0   0   0   0  21   0  40   0   0   0  21   0   0   0\n",
      "  104  20   0   0  41   0]\n",
      " [  0  17   0  39   0   3   0   0   0  41   0   0   0   0   0   0  56   0\n",
      "    0  80  30   0   0   0]\n",
      " [  0  11   0   3   0  35   0   0   0  16   0   0   0   0  19   0  32   0\n",
      "    0  50 131  32   0  17]\n",
      " [  0  20   0   0   0   3   0   0   0  20   0   0   0   0   0   0  22   0\n",
      "    0  17   1 123   0   0]\n",
      " [  0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   8  24\n",
      "   20   0   0  48 166   0]\n",
      " [  0   0   0  17   0   0   0   0  17   0   9   0   0   0   0   0  63   9\n",
      "   42   0   2  22   0 151]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.98      0.89       331\n",
      "           1       0.88      0.81      0.84       432\n",
      "           2       0.87      0.85      0.86       310\n",
      "           3       0.62      0.62      0.62       245\n",
      "           4       0.78      0.87      0.83       498\n",
      "           5       0.66      0.81      0.73       247\n",
      "           6       0.79      0.58      0.67       348\n",
      "           7       0.95      0.80      0.87       436\n",
      "           8       0.65      0.66      0.66       288\n",
      "          10       0.49      0.48      0.48       331\n",
      "          11       0.73      1.00      0.85       209\n",
      "          12       0.59      0.34      0.43       394\n",
      "          13       0.51      0.48      0.50       291\n",
      "          14       0.73      0.56      0.63       246\n",
      "          15       0.89      0.93      0.91       347\n",
      "          16       0.49      0.85      0.62       164\n",
      "          17       0.13      0.28      0.18       144\n",
      "          18       0.13      0.17      0.15       246\n",
      "          19       0.37      0.42      0.40       248\n",
      "          20       0.38      0.30      0.34       266\n",
      "          21       0.77      0.38      0.51       346\n",
      "          22       0.45      0.60      0.51       206\n",
      "          23       0.59      0.62      0.60       267\n",
      "          24       0.60      0.45      0.52       332\n",
      "\n",
      "    accuracy                           0.63      7172\n",
      "   macro avg       0.62      0.62      0.61      7172\n",
      "weighted avg       0.66      0.63      0.64      7172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class CustomLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000, num_classes=None):\n",
    "        \"\"\"\n",
    "        Initialize the Logistic Regression model\n",
    "        \n",
    "        Parameters:\n",
    "        - learning_rate: step size for gradient descent\n",
    "        - num_iterations: number of training iterations\n",
    "        - num_classes: number of unique classes in the dataset\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.num_classes = num_classes\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.class_mapping = None  # To handle zero-based indexing\n",
    "\n",
    "    def _softmax(self, z):\n",
    "        \"\"\"\n",
    "        Softmax activation function for multiclass classification\n",
    "        Prevents numerical instability by subtracting max value\n",
    "        \n",
    "        Parameters:\n",
    "        - z: input array of logits\n",
    "        \n",
    "        Returns:\n",
    "        - Softmax probabilities\n",
    "        \"\"\"\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def _one_hot_encode(self, y):\n",
    "        \"\"\"\n",
    "        Convert labels to one-hot encoded format\n",
    "        \n",
    "        Parameters:\n",
    "        - y: original labels\n",
    "        \n",
    "        Returns:\n",
    "        - One-hot encoded labels\n",
    "        \"\"\"\n",
    "        # Remap labels to zero-based index if needed\n",
    "        if self.class_mapping is None:\n",
    "            unique_classes = np.unique(y)\n",
    "            self.class_mapping = {orig: idx for idx, orig in enumerate(unique_classes)}\n",
    "            self.reverse_mapping = {idx: orig for orig, idx in self.class_mapping.items()}\n",
    "        \n",
    "        # Map original labels to zero-based index\n",
    "        y_mapped = np.array([self.class_mapping[label] for label in y])\n",
    "        \n",
    "        # Create one-hot encoding\n",
    "        one_hot = np.zeros((y.shape[0], self.num_classes))\n",
    "        one_hot[np.arange(y.shape[0]), y_mapped] = 1\n",
    "        return one_hot\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the logistic regression model\n",
    "        \n",
    "        Parameters:\n",
    "        - X: input features (num_samples, num_features)\n",
    "        - y: target labels\n",
    "        \"\"\"\n",
    "        # Determine number of classes if not specified\n",
    "        unique_classes = np.unique(y)\n",
    "        self.num_classes = len(unique_classes)\n",
    "        \n",
    "        # Initialize weights and bias\n",
    "        num_features = X.shape[1]\n",
    "        self.weights = np.zeros((num_features, self.num_classes))\n",
    "        self.bias = np.zeros((1, self.num_classes))\n",
    "        \n",
    "        # One-hot encode labels\n",
    "        Y_one_hot = self._one_hot_encode(y)\n",
    "        \n",
    "        # Gradient descent\n",
    "        for _ in range(self.num_iterations):\n",
    "            # Forward pass\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self._softmax(linear_model)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw = (1/X.shape[0]) * np.dot(X.T, (y_predicted - Y_one_hot))\n",
    "            db = (1/X.shape[0]) * np.sum(y_predicted - Y_one_hot, axis=0, keepdims=True)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on input data\n",
    "        \n",
    "        Parameters:\n",
    "        - X: input features\n",
    "        \n",
    "        Returns:\n",
    "        - Predicted class labels (original class labels)\n",
    "        \"\"\"\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self._softmax(linear_model)\n",
    "        \n",
    "        # Get indices of max probabilities\n",
    "        predicted_indices = np.argmax(y_predicted, axis=1)\n",
    "        \n",
    "        # Map back to original class labels\n",
    "        return np.array([self.reverse_mapping[idx] for idx in predicted_indices])\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities\n",
    "        \n",
    "        Parameters:\n",
    "        - X: input features\n",
    "        \n",
    "        Returns:\n",
    "        - Predicted class probabilities\n",
    "        \"\"\"\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        return self._softmax(linear_model)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute model accuracy\n",
    "        \n",
    "        Parameters:\n",
    "        - X: input features\n",
    "        - y: true labels\n",
    "        \n",
    "        Returns:\n",
    "        - Accuracy score\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "\n",
    "def preprocess_sign_mnist(train_data, test_data):\n",
    "    \"\"\"\n",
    "    Preprocess Sign MNIST dataset\n",
    "    \n",
    "    Parameters:\n",
    "    - train_data: Training dataframe\n",
    "    - test_data: Testing dataframe\n",
    "    \n",
    "    Returns:\n",
    "    - Preprocessed X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    # Separate features and labels\n",
    "    X_train = train_data.drop('label', axis=1).values\n",
    "    y_train = train_data['label'].values\n",
    "    \n",
    "    X_test = test_data.drop('label', axis=1).values\n",
    "    y_test = test_data['label'].values\n",
    "    \n",
    "    # Normalize pixel values to [0, 1]\n",
    "    X_train = X_train.astype('float32') / 255.0\n",
    "    X_test = X_test.astype('float32') / 255.0\n",
    "    \n",
    "    print(\"Unique training classes:\", np.unique(y_train))\n",
    "    print(\"Unique testing classes:\", np.unique(y_test))\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_logistic_regression(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train custom Logistic Regression on Sign MNIST\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Training features\n",
    "    - X_test: Testing features\n",
    "    - y_train: Training labels\n",
    "    - y_test: Testing labels\n",
    "    \n",
    "    Returns:\n",
    "    - Trained model\n",
    "    - Training and test accuracies\n",
    "    \"\"\"\n",
    "    # Create and train the model\n",
    "    clf = CustomLogisticRegression(\n",
    "        learning_rate=0.1,  # You can tune this\n",
    "        num_iterations=1000,  # You can increase for better convergence\n",
    "        num_classes=len(np.unique(y_train))\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Compute accuracies\n",
    "    train_accuracy = clf.accuracy(X_train, y_train)\n",
    "    test_accuracy = clf.accuracy(X_test, y_test)\n",
    "    \n",
    "    return clf, train_accuracy, test_accuracy\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    # Load the training dataset\n",
    "    train_csv_path = './archive/sign_mnist_train.csv'\n",
    "    train_data = pd.read_csv(train_csv_path)\n",
    "    \n",
    "    # Load the testing dataset\n",
    "    test_csv_path = './archive/sign_mnist_test.csv'\n",
    "    test_data = pd.read_csv(test_csv_path)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    X_train, X_test, y_train, y_test = preprocess_sign_mnist(train_data, test_data)\n",
    "    \n",
    "    # Train the model\n",
    "    model, train_acc, test_acc = train_logistic_regression(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Training Accuracy: {train_acc * 100:.2f}%\")\n",
    "    print(f\"Testing Accuracy: {test_acc * 100:.2f}%\")\n",
    "    \n",
    "    return model, X_test, y_test\n",
    "\n",
    "# Optional: Confusion Matrix and Classification Report\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Generate detailed model evaluation\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained logistic regression model\n",
    "    - X_test: Test features\n",
    "    - y_test: Test labels\n",
    "    \"\"\"\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    model, X_test, y_test = main()\n",
    "    evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e66ec64-5f68-4598-854d-c4e6bac43d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sign_mnist(train_data, test_data):\n",
    "    # Separate features and labels\n",
    "    X_train = train_data.drop('label', axis=1).values\n",
    "    y_train = train_data['label'].values\n",
    "    \n",
    "    X_test = test_data.drop('label', axis=1).values\n",
    "    y_test = test_data['label'].values\n",
    "    \n",
    "    # Normalize pixel values to [0, 1]\n",
    "    X_train = X_train.astype('float32') / 255.0\n",
    "    X_test = X_test.astype('float32') / 255.0\n",
    "    \n",
    "    print(\"Unique training classes:\", np.unique(y_train))\n",
    "    print(\"Unique testing classes:\", np.unique(y_test))\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c2e8240-c8a8-4a13-828a-14d951e934a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique training classes: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n",
      "Unique testing classes: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "X_train, X_test, y_train, y_test = preprocess_sign_mnist(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd075c60-e052-4056-8015-b6705f92e653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "clf = CustomLogisticRegression(\n",
    "    learning_rate=0.1,  # You can tune this\n",
    "    num_iterations=1000,  # You can increase for better convergence\n",
    "    num_classes=len(np.unique(y_train))\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "485248d3-6425-49d7-9059-a7827bb7f2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 76.39%\n",
      "Testing Accuracy: 63.43%\n"
     ]
    }
   ],
   "source": [
    "# Compute accuracies\n",
    "train_accuracy = clf.accuracy(X_train, y_train)\n",
    "test_accuracy = clf.accuracy(X_test, y_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Testing Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1444857a-3715-45cd-96f8-2f56bbf0b193",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[323   0   0   0   0   0   0   0   0   0   0   0   3   4   0   0   0   1\n",
      "    0   0   0   0   0   0]\n",
      " [  0 349   0   0   0   0   0   0   0  75   0   0   0   0   0   0   0   0\n",
      "    0   0   0   8   0   0]\n",
      " [  0   0 264   0   0  16   0   0   0   0   5   0   0  19   0   0   0   0\n",
      "    0   0   0   0   6   0]\n",
      " [  0   0   0 153   0   0   0   0   0  15   0   0  14   0   0   0   0   5\n",
      "    0   0   6   0  52   0]\n",
      " [  0   0   0   0 435   0   0   0   0   0   0   0   0   0   0   0   0  63\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0  20   0   0 201   0   0   0   0   1   0   0   5   0   0   0   0\n",
      "    0   0   0  20   0   0]\n",
      " [  0   0   0  20   0   0 203  18   0   0   0   0  21  14   0  19   0   0\n",
      "   52   0   0   0   1   0]\n",
      " [  0   0   0   0  16   0  45 348   0   0   0   0   0   0   0   0   0   0\n",
      "   22   0   0   0   5   0]\n",
      " [  3   0   0   0   0   0   0   0 190   0   0   0  21   0   0  12   0  21\n",
      "    0   0   0   0   0  41]\n",
      " [  0   0   0   9   0  21   0   0  24 159   0   0   0   0   0   0  87   3\n",
      "    0   0   0   8   0  20]\n",
      " [  0   0   0   0   0   0   0   0   0   0 209   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [ 24   0   0   0  47   0   0   0   0   0   0 135  45   0   0  39   0 104\n",
      "    0   0   0   0   0   0]\n",
      " [ 42   0   0   4  14   0   0   0   0   0   0  15 139  10   0  33   0   2\n",
      "   32   0   0   0   0   0]\n",
      " [  0   0  17   0  21  21   8   0   0   0   0   0   0 138   0  20   0   0\n",
      "    5   4   0   0  12   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 324  22   0   0\n",
      "    1   0   0   0   0   0]\n",
      " [  0   0   0   0   0   3   0   0   0   0   0  21   0   0   0 140   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  21   0   0   0   0   0  41  62\n",
      "    0  20   0   0   0   0]\n",
      " [  0   0   0   3  22   0   0   0  40   0   0  57  27   0   0   3   0  43\n",
      "    0  18   0  12   0  21]\n",
      " [  0   0   1   0   0   0   0   0  21   0  40   0   0   0  21   0   0   0\n",
      "  104  20   0   0  41   0]\n",
      " [  0  17   0  39   0   3   0   0   0  41   0   0   0   0   0   0  56   0\n",
      "    0  80  30   0   0   0]\n",
      " [  0  11   0   3   0  35   0   0   0  16   0   0   0   0  19   0  32   0\n",
      "    0  50 131  32   0  17]\n",
      " [  0  20   0   0   0   3   0   0   0  20   0   0   0   0   0   0  22   0\n",
      "    0  17   1 123   0   0]\n",
      " [  0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   8  24\n",
      "   20   0   0  48 166   0]\n",
      " [  0   0   0  17   0   0   0   0  17   0   9   0   0   0   0   0  63   9\n",
      "   42   0   2  22   0 151]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.98      0.89       331\n",
      "           1       0.88      0.81      0.84       432\n",
      "           2       0.87      0.85      0.86       310\n",
      "           3       0.62      0.62      0.62       245\n",
      "           4       0.78      0.87      0.83       498\n",
      "           5       0.66      0.81      0.73       247\n",
      "           6       0.79      0.58      0.67       348\n",
      "           7       0.95      0.80      0.87       436\n",
      "           8       0.65      0.66      0.66       288\n",
      "          10       0.49      0.48      0.48       331\n",
      "          11       0.73      1.00      0.85       209\n",
      "          12       0.59      0.34      0.43       394\n",
      "          13       0.51      0.48      0.50       291\n",
      "          14       0.73      0.56      0.63       246\n",
      "          15       0.89      0.93      0.91       347\n",
      "          16       0.49      0.85      0.62       164\n",
      "          17       0.13      0.28      0.18       144\n",
      "          18       0.13      0.17      0.15       246\n",
      "          19       0.37      0.42      0.40       248\n",
      "          20       0.38      0.30      0.34       266\n",
      "          21       0.77      0.38      0.51       346\n",
      "          22       0.45      0.60      0.51       206\n",
      "          23       0.59      0.62      0.60       267\n",
      "          24       0.60      0.45      0.52       332\n",
      "\n",
      "    accuracy                           0.63      7172\n",
      "   macro avg       0.62      0.62      0.61      7172\n",
      "weighted avg       0.66      0.63      0.64      7172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Import sklearn metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b53ce136-6cdb-420e-9565-85470596e73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABb4AAAExCAYAAACzsrRmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABTXElEQVR4nO3deXxeZZ3//8+d3NnXNkmTlLbpQltoXQq0pWqlZWot0oJYdkcpUJk6KCPjIKPMV3FBxRlHQBQHBoVRq0DF4oAsDoiAKEuhBQukK226Jc2+r/d9fn/waH6EtL3eba/cuXN4PR8P/zC8c33Ofc51Puc6V5s0EgRBYAAAAAAAAAAAhETKcB8AAAAAAAAAAAA+sfENAAAAAAAAAAgVNr4BAAAAAAAAAKHCxjcAAAAAAAAAIFTY+AYAAAAAAAAAhAob3wAAAAAAAACAUGHjGwAAAAAAAAAQKmx8AwAAAAAAAABChY1vAAAAAAAAAECosPGdpCZOnGiXXnrpcB8GABwTehmAZEaPAjAc6D0ARhr6FkYqNr4P4u6777ZIJNL/v8zMTJs2bZp9/vOft5qamuE+PMm3v/1tO/vss620tNQikYh9/etfP2T2nnvusZNPPtkyMzOtpKTEVq5caXV1dUdcc8eOHQPO2+H+t2PHjqP/cEPoJz/5iZ1//vk2YcIEi0Qih2zsTz/9tJ199tk2fvx4y8zMtLKyMjvjjDPs2WefTewBA4cRhl62b98++4d/+AebNGmSZWVl2ZQpU+yLX/yi1dfXH/FYCxculPrT4frlcLvvvvts3rx5VlhYaEVFRbZgwQL7/e9/P9yHBRyVkd6jvv71rx+2lxzpmmCk96jDHfPixYuH+/CAfiO991RWVtq1115rs2bNsry8PCsvL7elS5faunXrDprfs2ePXXDBBVZYWGj5+fn28Y9/3LZv337EdUf6u96uXbvsG9/4hs2dO9dGjRplxcXFtnDhQnv88ccPmm9qarJ/+Id/sJKSEsvJybHTTz/dXn755QQfNfCWkd633mn16tUWiUQsNzf3qL5/pK+ZXnjhBbvyyivtlFNOsbS0NItEIofMNjc327XXXmtTp061rKwsq6iosJUrV1pVVVUCj3hkiw73ASSzb37zmzZp0iTr6uqyP//5z/aTn/zEHn74Ydu4caNlZ2cP9+Ed1v/7f//PysrK7KSTTrLHHnvskLmf/OQnduWVV9qiRYvsBz/4ge3evdtuueUWW7dunT3//POWmZkp1ywpKbFf/OIXA772n//5n7Z792676aabBmWT0fe+9z1rbW21uXPn2r59+w6Z27x5s6WkpNhnP/tZKysrs8bGRvvlL39pp512mv3+97+3M844I4FHDRzeSO1lbW1t9oEPfMDa29vtyiuvtPHjx9srr7xiP/rRj+zJJ5+0l156yVJS9D+//bd/+zf7zGc+0///X3zxRfvhD39o1113nZ144on9X3/f+97n9XP4cuutt9o//dM/2dKlS+3GG2+0rq4uu/vuu23ZsmV2//332/Lly4f7EIGjMlJ71PLly+34448f9PXrrrvO2trabM6cOUc03kjvUe9cA5qZrVu3zm655Rb76Ec/OgxHBBzeSO09d955p/30pz+1c88916688kprbm6222+/3ebNm2ePPvqofeQjH+nPtrW12emnn27Nzc123XXXWVpamt100022YMEC27BhgxUVFcl1R/q73u9+9zv73ve+Z+ecc46tWLHC+vr67Oc//7ktXrzYfvazn9lll13Wn43H47Z06VJ75ZVX7Etf+pIVFxfbbbfdZgsXLrSXXnrJpk6dOoyfBO9mI7VvvV1bW5tde+21lpOTc9RjjPQ108MPP2x33nmnve9977PJkyfb5s2bD5qLx+O2ePFie/311+3KK6+0adOm2datW+22226zxx57zN544w3Ly8tL8NGPQAEGueuuuwIzC1588cUBX//iF78YmFnwq1/96pDf29bW5uUYKioqghUrVhz197/55ptBEARBbW1tYGbB9ddfPyjT3d0dFBYWBqeddloQj8f7v/7ggw8GZhb88Ic/POr6ByxdujSoqKg4bCYejwcdHR3HXMuHHTt29J+LnJycI7oG7e3tQWlpabBkyZIhOjrgyIz0XrZ69erAzIKHHnpowNe/9rWvBWYWvPzyy8d0bGvWrAnMLHjyyScPm/N1Lo7V1KlTgzlz5gzo183NzUFubm5w9tlnD+ORAUdnpPeog6mqqgoikUhwxRVXHPNYI61HHczKlSuDSCQS7Nq1a7gPBeg30nvPunXrgtbW1gFfq6urC0pKSoIPfehDA77+ve99LzCz4IUXXuj/2htvvBGkpqYGX/nKV46q/tuNpHe9jRs3BrW1tQO+1tXVFZxwwgnBuHHjBnz93nvvDcwsWLNmTf/X9u/fHxQWFgYXX3xxQo4XeLuR3rfe7l//9V+D6dOnB3//938f5OTkHPuBBSNvzVRdXd3fFz/3uc8Fh9qaffbZZwMzC370ox8N+PrPfvazwMyC3/72t0N+rGHArzo5An/3d39nZmZvvvmmmZldeumllpuba9u2bbMzzzzT8vLy7O///u/N7K0/mbn55ptt5syZlpmZaaWlpbZq1SprbGwcMGYQBHbDDTfYuHHjLDs7204//XR77bXXDlp/27Zttm3bNulYJ06c6Mxs3LjRmpqa7MILLxzwoxXLli2z3Nxcu+eee6RaR2rixIm2bNkye+yxx2z27NmWlZVlt99+e/+Pz919992DvudgP6ayZ88eu/zyy620tNQyMjJs5syZ9rOf/WzQ91ZVVVllZaV0bBUVFYf9MZPDyc7OtpKSEmtqajqq7wcSZaT0spaWFjMzKy0tHfD18vJyMzPLyso6gk+tOfCrC15//XX75Cc/aaNGjbL58+eb2Vs/Urdw4cJB33PppZcO6rnqeWtubrbKykprbm52HltLS4uNGTNmQI/Kz8+33NzcITkXwHAZKT3qYH79619bEAT9x+dbMveod+ru7rb777/fFixYYOPGjTvi7wcSbaT0nlNOOWXQrwcoKiqyD3/4w/bGG28M+PpvfvMbmzNnzoCfQDnhhBNs0aJFdt999zlrHY1kfdebOXOmFRcXD/haRkaGnXnmmbZ7925rbW3t//pvfvMbKy0tHfDTdCUlJXbBBRfY7373O+vu7nbWAxJhpPStA7Zs2WI33XST/eAHP7BodGh/AUUyr5lKS0ul97fheB8OIza+j8CBG/rtPxLW19dnS5YssTFjxtj3v/99O/fcc83MbNWqVfalL33JPvShD9ktt9xil112ma1evdqWLFlivb29/d//ta99zb761a/a+9//fvuP//gPmzx5sn30ox+19vb2QfUXLVpkixYt8vZ5DjywD3azZGVl2fr16y0ej3ur93abNm2yiy++2BYvXmy33HKLzZo164i+v6amxubNm2ePP/64ff7zn7dbbrnFjj/+eFu5cqXdfPPNA7KXXHLJgB938amlpcXq6uqssrLSrrvuOtu4caPXawQMhZHSy0477TRLSUmxL3zhC/bcc8/Z7t277eGHH7Zvf/vbds4559gJJ5xwrKfikM4//3zr6Oiw73znO3bFFVcc8fer523t2rV24okn2tq1a51jLly40B599FG79dZbbceOHVZZWWmf+9znrLm52b7whS8c8TECyWqk9KiDWb16tY0fP95OO+20o/p+VTL2qHd6+OGHrampacj+EADwbST3HjOz6urqARu78XjcXn31VZs9e/ag7Ny5c23btm0DNnt9GknvetXV1ZadnT3g10SsX7/eTj755EG/Um/u3LnW0dFxyF9LACTaSOtbV199tZ1++ul25plnHu1HPmIjYc10KLNnz7acnBz76le/an/84x9tz5499tRTT9m1115rc+bMGfCrrXAYw/nXzZPVgR8jefzxx4Pa2tpg165dwT333BMUFRUFWVlZwe7du4MgCIIVK1YEZhZ8+ctfHvD9zzzzTGBmwerVqwd8/dFHHx3w9f379wfp6enB0qVLB/zo+nXXXReY2aAfI6moqHD+KNk7He5XndTW1gaRSCRYuXLlgK9XVlYGZhaYWVBXV3dE9d7pYD/+VlFREZhZ8Oijjw74+ptvvhmYWXDXXXcNGuedn2HlypVBeXn5oOO76KKLgoKCggE/TrdgwYJD/ujI4Si/6mTJkiX95yo9PT1YtWpV0NnZecS1gKEQhl525513BoWFhf332YHxent7j/BsDHawH4m7/vrrAzM76I+xLliwIFiwYMGgr69YsWLA51HPWxD8/9foYH3vnWpqaoJFixYNOBfFxcXBX/7yF+f3AskoDD3q7TZu3BiYWXDttdce8fcezEjrUe907rnnBhkZGUFjY+MRfy8wlMLWe4IgCJ5++ukgEokEX/3qV/u/duA98Jvf/Oag/I9//OPAzILKysqjqnfASH7XC4Ig2LJlS5CZmRl8+tOfHvD1nJyc4PLLLx+U//3vf3/QzwYMtTD0rYceeiiIRqPBa6+91n+sQ/mrTkbKmulwv+okCN46b+Xl5QPeAZcsWTLoV17h0Pgb34fxkY98xEpKSmz8+PF20UUXWW5urq1du9aOO+64Abl//Md/HPD/16xZYwUFBbZ48WKrq6vr/9+BH0t78sknzczs8ccft56eHrvqqqsG/Oj61VdffdDj2bFjh9d/Ibu4uNguuOAC+5//+R/7z//8T9u+fbs988wzduGFF1paWpqZmXV2dnqr93aTJk2yJUuWHNX3BkFg999/v5111lkWBMGAc7xkyRJrbm4e8C9u/+lPf7IgCHwd+gA33nij/eEPf7Cf/vSnNm/ePOvp6bG+vr4hqQUcrZHcy4477jibO3eu3XzzzbZ27Vr74he/aKtXr7Yvf/nL+gk4Cp/97GeP+nvV82b21o/TBUFgl156qXPc7Oxsmz59uq1YscLWrFljP/vZz6y8vNyWL19uW7duPerjBYbbSO5Rb7d69Wozs4T8Dedk7FFv19LSYr///e/tzDPPtMLCwqM+VmAohaX37N+/3z75yU/apEmT7Nprr+3/+oH3uIyMjEHfk5mZOSDj20h41+vo6LDzzz/fsrKy7MYbbxzw3zo7O4flvAEuI7Vv9fT02D//8z/bZz/7WZsxY8aRfehjlOxrJpeSkhI76aST7Nvf/rY98MAD9vWvf92eeeaZAf8gLw5vaH+pzgj34x//2KZNm2bRaNRKS0tt+vTpg37cKRqNDvq9hVu2bLHm5mYbM2bMQcfdv3+/mZnt3LnTzGzQvwpdUlJio0aN8vUxDuv222+3zs5Ou+aaa+yaa64xM7NPfepTNmXKFPvtb3876PfH+TJp0qSj/t7a2lpramqyO+64w+64446DZg6c46H29h/b+9SnPmUnn3yyXXrppfab3/wmIfUBxUjtZc8++6wtW7bMnnvuuf4f0z3nnHMsPz/fvvGNb9jll18+ZAunY+lR6nk7Uueff75Fo1F78MEH+7/28Y9/3KZOnWr/9m//Zvfee+9RjQsMt5Hao94uCAL71a9+Ze95z3vsfe97n5cxDycZe9Tb3X///dbV1cWvOUFSC0PvaW9vt2XLlllra6v9+c9/HvDuduDXWR7s91F3dXUNyPiW7O96sVjMLrroInv99dftkUcesbFjxw7471lZWcNy3gCXkdq3brrpJqurq7NvfOMbRz3G0Ur2NdPhbN++3U4//XT7+c9/3v8raz7+8Y/bxIkT7dJLL7VHHnnEPvaxjw3pMYQBG9+HMXfu3IP+TrS3y8jIGNRo4vG4jRkzpv9v/rxTSUmJt2M8VgUFBfa73/3OqqqqbMeOHVZRUWEVFRX2wQ9+0EpKSobsb+kcbLFwqH9UMhaLDfj/B37v+Kc+9SlbsWLFQb8nES+d75Senm5nn3223XjjjdbZ2cmCCEljpPay22+/3UpLSwcd+9lnn21f//rX7S9/+cuQbXwfqkcd7G8UHaxH+T5v27dvt0cffXTQC+Do0aNt/vz59uyzzx7xmECyGKk96u2effZZ27lzp333u99NSL1k61HvtHr1aisoKLBly5Yd81jAUBnpvaenp8eWL19ur776qj322GP2nve8Z8B/Hz16tGVkZNi+ffsGfe+Br71zw9eXZH/Xu+KKK+yhhx6y1atX9//jgG9XXl4+LOcNcBmJfau5udluuOEGu/LKK62lpaX/H2xsa2uzIAhsx44dlp2dfcjN5WOV7Gumw7n77rutq6tr0Hrq7LPPNrO31p9sfLux8T0EpkyZYo8//rh96EMfOuzmZ0VFhZm99adIkydP7v96bW3toH8hdqhNmDDBJkyYYGZmTU1N9tJLL/X/iVKiHPgTxKampgFfP/CnjgeUlJRYXl6exWKxpPtl/p2dnRYEgbW2trLxjRFvuHtZTU3NoMWHmfX/IyKJ/rVCo0aNsu3btw/6+jt7lHrejkRNTY2ZDV6Mmb11PvgVS3g3Gu4e9XarV6+2SCRin/zkJ72MdzSGs0e93b59++zJJ5+0Sy+99KC/KgAY6ZKh98TjcbvkkkvsiSeesPvuu88WLFgwKJOSkmLvfe97bd26dYP+2/PPP2+TJ0+2vLy8YzqOI5Es73pf+tKX7K677rKbb77ZLr744oNmZs2aZc8884zF4/EBG4jPP/+8ZWdn27Rp07wfFzCUhrNvNTY2Wltbm/37v/+7/fu///ug/z5p0iT7+Mc/bg888MBRjX80kmXN5FJTU2NBEAx6Bxyu9+GRit/xPQQuuOACi8Vi9q1vfWvQf+vr6+t/2H/kIx+xtLQ0u/XWWwf8adM7/6XqA7Zt29b/r/YOpa985SvW19dn//zP/zzktd4uPz/fiouL7emnnx7w9dtuu23A/09NTbVzzz3X7r//ftu4ceOgcWprawf8/6qqKqusrPR6rAf7kZampia7//77bfz48UP2p5VAIg13L5s2bZrV1NTYn/70pwFf//Wvf21mZieddJL2QTyZMmWKVVZWDugxr7zyyqC/ba2eN7O3/gZEZWWlNTc3H7b28ccfbykpKXbvvfcOOMe7d++2Z555JuHnAkgGw92jDujt7bU1a9bY/Pnz+/8SwXAYzh71dvfcc4/F43F+zQlCKxl6z1VXXWX33nuv3XbbbbZ8+fJD5s477zx78cUXB2x+b9q0yf74xz/a+eefL9XyJRne9f7jP/7Dvv/979t1111nX/jCFw6ZO++886ympsZ++9vf9n+trq7O1qxZY2eddRZ/qIcRZzj71pgxY2zt2rWD/nf66adbZmamrV271r7yla8c9Wc7GsmyZnKZNm2aBUFg991334CvD9f78EjF3/geAgsWLLBVq1bZd7/7XduwYYN99KMftbS0NNuyZYutWbPGbrnlFjvvvPOspKTErrnmGvvud79ry5YtszPPPNPWr19vjzzyiBUXFw8ad9GiRWZm0j8e8Itf/MJ27txpHR0dZmb29NNP2w033GBmZp/+9Kf7/yTvxhtvtI0bN9qpp55q0WjUHnjgAfvDH/5gN9xwg82ZM2fAmAsXLrSnnnpqyP6hSDOzz3zmM3bjjTfaZz7zGZs9e7Y9/fTTtnnz5kG5G2+80Z588kk79dRT7YorrrAZM2ZYQ0ODvfzyy/b4449bQ0NDf/aSSy6Rj/vBBx+0V155xczeepF99dVX+8/b2Wef3f9jdR/72Mds3Lhxduqpp9qYMWOsqqrK7rrrLtu7dy+/ZxehMdy97POf/7zddddddtZZZ9lVV11lFRUV9tRTT9mvf/1rW7x4sZ166qn92bvvvtsuu+wyu+uuu7z/gyIHXH755faDH/zAlixZYitXrrT9+/fbf/3Xf9nMmTP7f2TPTD9vZmZr166VjrukpMQuv/xyu/POO23RokW2fPlya21ttdtuu806OzsTvlgEksFw96gDHnvsMauvrz/sRm/Ye9TbrV692saOHWsLFy4cgk8JDL/h7j0333yz3XbbbfaBD3zAsrOz7Ze//OWA//6JT3zCcnJyzMzsyiuvtP/+7/+2pUuX2jXXXGNpaWn2gx/8wEpLS+1f/uVfBnxf2N/11q5da9dee61NnTrVTjzxxEHnbfHixVZaWmpmb218z5s3zy677DJ7/fXXrbi42G677TaLxWLD8juKgWM1nH0rOzvbzjnnnEFff+CBB+yFF14Y9N/eDWumnTt32i9+8Qszs/4/mDyw71RRUWGf/vSnzeytfzDz+9//vq1atcrWr19vM2fOtJdfftnuvPNOmzlzpn3iE5/wfWrCKcAgd911V2BmwYsvvnjY3IoVK4KcnJxD/vc77rgjOOWUU4KsrKwgLy8veO973xtce+21wd69e/szsVgs+MY3vhGUl5cHWVlZwcKFC4ONGzcGFRUVwYoVKwaMV1FREVRUVEifYcGCBYGZHfR/Tz75ZH/uoYceCubOnRvk5eUF2dnZwbx584L77rvvoGOecsopQVlZmVT/gKVLlw465oqKimDp0qUHzXd0dAQrV64MCgoKgry8vOCCCy4I9u/fH5hZcP311w/I1tTUBJ/73OeC8ePHB2lpaUFZWVmwaNGi4I477jjouVCsWLHikOftrrvu6s/96Ec/CubPnx8UFxcH0Wg0KCkpCc4666zg6aefluoAiRCGXlZZWRmcd955/fd5RUVFcM011wTt7e0DcrfeemtgZsGjjz4qjRsEQbBmzZpBPfH6668PzCyora096Pf88pe/DCZPnhykp6cHs2bNCh577LFgxYoVB/08ynk7cI3e3l8Opbe3N7j11luDWbNmBbm5uUFubm5w+umnB3/84x/lzwwkkzD0qCAIgosuuihIS0sL6uvrD5l5N/SoIHirZ5tZ8MUvflH+nECijfTec7j3FTML3nzzzQH5Xbt2Beedd16Qn58f5ObmBsuWLQu2bNkyaNywv+sd6J/KO3IQBEFDQ0OwcuXKoKioKMjOzg4WLFjgnDPAUBnpfetIjvXdsGZ68sknD9mLFixYMCC7e/fu4PLLLw8mTZoUpKenB+Xl5cEVV1xxyM+FwSJBMIR/pIvQaG1ttdGjR9vNN99sn/vc54b7cABggAsuuMB27NhhL7zwwnAfCgAMQo8CkMx41wOQLFgzwTd+1QkkTz/9tB133HF2xRVXDPehAMAAQRDYn/70p0E/sgoAyYAeBSDZ8a4HIBmwZsJQ4G98AwAAAAAAAABCJWW4DwAAAAAAAAAAAJ/Y+AYAAAAAAAAAhAob3wAAAAAAAACAUGHjGwAAAAAAAAAQKmx8AwAAAAAAAABCJaoG//znPzszKSnaPnpqaqqXjFozEol4GUel1PNdUxGPx52ZRB9Tb2+vlAuCwEs95Ryo1GOKxWLOTF9fn7d6ymdUz4NS8+/+7u+ksYbaDTfc4MxkZGRIY6WlpTkz6enp0ljKPaWMpfZEJZfoXq32REWie5TKV2/xeZ+rYynXR+lRPvurzx61YsWKYz0cL5577jlnJhrVlmW+1j5qTqk3HPemUtPnvEw0X2sfdSxlLihrGpX6+RK9BlTq+Rxr3rx50lhD7ac//akzo/Yon2sRpaa6RvIl0fVUvvpdsj7PffUxM+0d1OexK73T57ulSqm5atUqb/WOxauvvurM+Hzn8Pnu5fO54nO95WsPwud6xef8TvQaUKmnfj5ffUWljJXonqjmzjjjDGcmOXcQAAAAAAAAAAA4Smx8AwAAAAAAAABChY1vAAAAAAAAAECosPENAAAAAAAAAAgVNr4BAAAAAAAAAKHCxjcAAAAAAAAAIFTY+AYAAAAAAAAAhAob3wAAAAAAAACAUIn6HCwlxd8+empqqreakUjkWA/niOr5FI26L5F6TMpYHR0dzox6bZRcenq6NFZnZ6eUc1HnQiwW8zaWch6UsYIgkOr19fV5qWemnYdkoZxnde76vO98XX+f883nPeyzJyqf0WfP9zm/1XPqi9IP4vG4t7GUXq3W83ne1ZrJwOf9NBzrLZdEr4/MtHtd+Xzq8zXREj2/lfPgs9f57AXKsatzVDnv6nlI1rl1MD7XIso6SqVcN5/z0ldfUSnzTZ27yj3l8z5X3jl83nfKsdfX10v18vPznZlE9wy15/vc23i39qhE9xVf74Mqde4qNZW+4vOdw+fzI9F7LMp599nPfa6jfM4/n3wdF3/jGwAAAAAAAAAQKmx8AwAAAAAAAABChY1vAAAAAAAAAECosPENAAAAAAAAAAgVNr4BAAAAAAAAAKHCxjcAAAAAAAAAIFTY+AYAAAAAAAAAhAob3wAAAAAAAACAUGHjGwAAAAAAAAAQKlE5GHVHU1NTpbEikYha1lvNZKynnIe6ujpnZtOmTVK97OxsZ+aDH/ygM1NVVSXVe+2115yZ/Px8aazZs2c7M11dXc6Mz+sXBIG3sZS5EIvFpLGUe7Wvr08aayRRzmFKivZnfUpO7WPK9VDqKeOYmaWlpUk5hXK/+DxXCp/X2edxxePxhNZT+o9aT+kt6jlVKMfus16yUK6Hes183ne+novDcey+qM9z5T73OXeV8+BzLaJ8PpVyXOrcU9c/Luq58nkNfZ7Toebz+apI9LM60T1Rpazv1HmU6HPl851DmQ+jR492Zh544AGpnvIOeu6550pj1dbWSjkXde4p80Htm4l+3h4Ln+84ycjn5/P13DQzS09P9zaWMseVY1c/X3d3tzPT3t4ujTVq1Chnpqenx5lR3+mVezMzM1Maq6WlxZlRerD63Pb5XuxrLofvzRIAAAAAAAAA8K7GxjcAAAAAAAAAIFTY+AYAAAAAAAAAhAob3wAAAAAAAACAUGHjGwAAAAAAAAAQKmx8AwAAAAAAAABChY1vAAAAAAAAAECosPENAAAAAAAAAAiVqM/BIpGIlEtJce+3p6amHuvhHBG1XjTqPmWxWEwaKzMz05kJgsCZ6ezslOqtX7/emWltbXVmcnNzpXq7d+92Zp566ilprKuvvtqZmT17tjPT3Nws1VPmg3JtzMzi8biUc1HuG7WeMo/NzHp7e6VcMlDOj88elZaWJo2lzCXleqg9SvmM6rErlHOlzl0156Lec8p5V8fydew+qc8i5diVsXw+t9V7Vf2MyUA5P+rnVnLqWL7qqc8V5dmpHrv6HPZVT/mMyjH5Om4zv9fZZx/ztfYx045LqZfoeWWWnM+GQ0l0D1fPja9+5/Pz+byuvua3Olaiqc9pZW2qXOe2tjapXnt7uzOzefNmaayCggJnJj093Znp6+uT6vm8zj77XTLw+a7n85mR6P7q8zwon2/dunVSvTfeeMOZ+ehHP+rMKPecmdkTTzzhzCh7ZGZmkydPdmYuueQSZ2b79u1SvcrKSmemqKhIGuuEE05wZrq6upwZn++WKl9jJd/TEQAAAAAAAACAY8DGNwAAAAAAAAAgVNj4BgAAAAAAAACEChvfAAAAAAAAAIBQYeMbAAAAAAAAABAqbHwDAAAAAAAAAEKFjW8AAAAAAAAAQKiw8Q0AAAAAAAAACBU2vgEAAAAAAAAAoRJVg5FIxJlJSdH20VNTU72N5auekjHTjisWi0ljZWdnOzOTJk1yZurq6qR6nZ2dzsw999zjzFxwwQVSvbKyMmcmLy9PGuuZZ55xZubPn+/MtLa2SvV8Uu4dhTqvlDkaj8elsdT7IhlEo+52pn4e5ZqpYynHpVDvlT179jgz3d3d0lgnnniiM9Pb2yuNpfDV930+P9SxlHvK53Gp97CvsZR7IggCqZ5y76j9biTx2VeUnPrsUealzzWgT8pxKXMp0femem18rR/MtPPg8z73uRbxee8owth/FMo183mvJLpHqZR6iZ5vie6vPtcY6rHn5OQ4M42Njc6Mui7dvHmzM9PX1yeNVV5e7sycc845zkxLS4tUzyef985Q8/ksUJ9lCl/veiqfe2lZWVnOTHNzszNTU1Mj1VPuuxdffNGZueiii6R6GRkZzox6nyv7ZF1dXc7M6NGjpXp/+9vfnBnlfJqZnX/++c7MsmXLnBl13zEZ8Te+AQAAAAAAAAChwsY3AAAAAAAAACBU2PgGAAAAAAAAAIQKG98AAAAAAAAAgFBh4xsAAAAAAAAAECpsfAMAAAAAAAAAQoWNbwAAAAAAAABAqLDxDQAAAAAAAAAIlagaTElx75FHIhFpLDWnSE1N9VIv0cdkZtbR0eHMZGRkeKvX2dnpzPT29jozmzdvluqVl5c7M0EQSGPt27fPmYnFYs5MNKpNeeW4lHoq5f5SJetxDTXlWNXrn5aW5swkut8VFRVJuS1btjgzTzzxhDTWtGnTnBml/4ykeXS01D7sEo/HpZzPc6qMpWTUY+/r65NyCp/P7qGmnEN1Hvla+6g5X/NbradSntXKsatrEWWOJ7rfqfedr/Og1lPGUs+VWtNFvc7JuiZLBuo6Splval9Reoav55iaU/uYr7HUuavwOZZyDdV7IDc315lR3gcbGhqkehMnTnRmpkyZIo21detWKefic476XGslC59rEZ9j+aqnXv9t27Y5My+//LI0lnIfzJgxw5nJz8+X6invls8995wzs2nTJqneWWed5cysW7dOGks59nvuuceZufDCC6V6BQUFzozaX3/84x87M/Pnz3dmlL1JM7/9x9caMPy7EQAAAAAAAACAdxU2vgEAAAAAAAAAocLGNwAAAAAAAAAgVNj4BgAAAAAAAACEChvfAAAAAAAAAIBQYeMbAAAAAAAAABAqbHwDAAAAAAAAAEKFjW8AAAAAAAAAQKhEfQ6WkqLtoyu5aFQ7NGWsSCTiZRx1rKysLGkspWZzc7Mz09vbK9Xr6OhwZtLS0pyZzZs3S/U6OzudmaamJmks5bzH43FpLF/1VMp1Vo49NTXVx+EckVgslvCaR0s5P+o5VHJqz1DG6uvrc2bU+V1UVOTM7NmzRxpLyR1//PHOTE9Pj1RvOOZ4IinXUJ1XI5nyGYMg8DZWsvC1XlFz6li+eqd6/yrrO6UnmoX/nlI+n3qv+Kqnnk9l/aBeZ+Uz+lyv+DzvPteTQ025N33eTz57lHJcao9Sjkt9T/VVT51vvt6FfK7HfD4buru7nRn1XH34wx92Ztra2qSx1F7mos4r5TOqY/k69kTwuY7y+a7n677Lzc2VcsrzTj2mP/zhD86M8j54yimnSPV27NjhzBQXFzsz6n6Ucq4yMjKksZTrM2fOHGemsrJSqjd69GhnZuzYsdJYyvlSjmvu3LlSvZaWFmcm0e/9I/dtAAAAAAAAAACAg2DjGwAAAAAAAAAQKmx8AwAAAAAAAABChY1vAAAAAAAAAECosPENAAAAAAAAAAgVNr4BAAAAAAAAAKHCxjcAAAAAAAAAIFTY+AYAAAAAAAAAhAob3wAAAAAAAACAUImqwdTUVGcmEokc08EcDaVmSop7f1899iAInJnq6mpprPT0dGemubnZmYnFYlK9aNR9uYuKirwck5lZYWGhM6PMK99jKZQ5o1w/M/36JJIyj48kN1LE43FvubS0tGM9nH7K3O3r65PGGj9+/LEeTj+ll82YMcOZ8dmjlDmp1vP5XFNqKvV83nNqT1SOXbknlL6p8jlWGPla+6iU/lNfXy+N1dHR4czk5uZKYynP4a6uLmemvb1dqudrnZuZmellHDP92aDkMjIynJnW1lapnqK4uFjKKc8Gn+8gyr3j8zmTLHyeQ+VzJ3rdrtZLdH9Vnq/qtfF1DdW1iHIfqOtl5TzU1dU5M0q/MDObMmWKM7Np0yZpLPWd0EWdo8r1Ud95fL7PJIPh6LnKOVTuFfW5MmbMGGdm6tSp0ljKc/8vf/mLM1NSUiLVq6iocGaqqqqcmS1btkj1nnjiCWdm4sSJ0lg1NTXOTE9PjzOj7qWVl5c7M+p8V+ao2jMUyjPSZz0Fb5YAAAAAAAAAgFBh4xsAAAAAAAAAECpsfAMAAAAAAAAAQoWNbwAAAAAAAABAqLDxDQAAAAAAAAAIFTa+AQAAAAAAAAChwsY3AAAAAAAAACBU2PgGAAAAAAAAAIRKVA1GIpGhPI5BUlK0PXkll5qaeqyH06+mpsaZeeKJJ6Sxdu3a5cxkZWU5M6eddppUb/bs2c5MU1OTMxMEgVQvJyfHmcnPz5fGysjIcGbS09OdGXVeKXMmHo9LYyX63lGox+Tz3hlqyrH6vP7qufF1XO3t7VK9tLQ0ZyYzM1Maq6qqystYXV1dUj1FNOp+bKmfr7u7+1gPp59ynZVr09PTI9VT+o/aq33N91gsJtVT+o967GofTgaJ7isq5Vw3NDQ4M1u3bpXq7dmzx5lRe3VBQYEzo6wz1P7a2dnpZazCwkKpXklJiTOj3gPK+k45Vzt37pTqjR8/3pkZN26cNNb27dudmYqKCmdGvW+UXqaONZJ6lNKbfb6fqZTnvlJPXfsqOXUsX8fl83wmen6rz3NlTaa8hxcXF0v1lD7c1tYmjaX0TuU9tbe3V6rnc86oa7dk4POd2uceknIOc3NznZndu3dL9ZR7avTo0dJYtbW1zkxZWZkzs3btWqneokWLnJns7GxnRtlnMjPbsGGDM3PGGWdIYyn3Z319vTOjXhtlXqnvskqvVvYdlXdZM+1eVe8vX88//sY3AAAAAAAAACBU2PgGAAAAAAAAAIQKG98AAAAAAAAAgFBh4xsAAAAAAAAAECpsfAMAAAAAAAAAQoWNbwAAAAAAAABAqLDxDQAAAAAAAAAIFTa+AQAAAAAAAAChwsY3AAAAAAAAACBUomowJcW9R65kzMyiUXfZSCQijaXmXAoKCqRcdXW1l3pmZhkZGc7Mm2++6cx8+MMfluqVlJQ4M/n5+c7Mpk2bpHoNDQ3OTF9fnzTWunXrnJmtW7c6M+PHj5fqdXZ2OjPqfA+CwJlR7gllHN9isVjCax4t5XqkpqZ6G0vtPUpN5fqr98rYsWOdmQULFkhjvfrqq85MT0+PM5OWlibVU+bbrl27nBnl/jUzmzFjhjPT3d0tjbVhwwZnRjlXCxculOopx6X2qK6uLi9jqfeEcp1HUu9R+TyHSj9Qz6Fyv7S3tzsz6jpK6YnK2sfMrLKy0pkpLS11ZjIzM6V6Sq/u7e11Zpqbm6V6yjpRvc7KnKmtrXVmlHNgZtbW1ubMVFVVSWM1NjY6MxUVFc6MugZQqOdd7cPJwOe7XrKONVLr+XrfNdN6wZ///GdpLKXvL168WBpL6T8tLS3OzJQpU6R6Sn+tq6uTxlL6Ynp6ujPT0dEh1VPmTDwel8YaSXzem8rzwOd9p8wR9X3piSeecGbKy8ulsZQ1S1NTkzPz17/+Var3nve8x5lR9qPUfZEdO3Y4M8qelZlZa2urlHNR3gfNtLW3cv3MtDXL3r17nRmlj5n57VG+etnIWY0BAAAAAAAAACBg4xsAAAAAAAAAECpsfAMAAAAAAAAAQoWNbwAAAAAAAABAqLDxDQAAAAAAAAAIFTa+AQAAAAAAAAChwsY3AAAAAAAAACBU2PgGAAAAAAAAAIRKVA1GIpGhPI5BUlK0PfkgCJyZtrY2Z+aZZ56R6qWnpzsz48ePl8bKyclxZvbv3+/M/PCHP5TqnXbaac7Mq6++6sx0dHRI9V5//XUppygsLHRm7r33Xmfmuuuuk+p1dnY6M+o9oeTU+a6IxWLexvJ5XMlAvWbK507Wc6P0qIqKCmmsRx55xJnZvXu3MzNu3Dip3tixY6Wcy0MPPSTllPt82bJl0liVlZXOzP/+7/86M319fVK93NxcZyYej0tjnXLKKc5MT0+PM6PeEz7vndTUVG9jDTXleijn2Ux7DqtjKX1RWa8oGTNtjjc2NkpjNTc3OzPKHMnPz5fqKed97969zkxeXp5Ub+vWrc5MRkaGNFZ2drYzo8wZdY2RlZXlzOzYsUMaa9asWc6M8uxT55Uyl9Xeo/b0ZJDotY/P/q0cl1pPGUtdTyo1lXtKve+UXGZmprd6yrvXmDFjpLGUd72GhgZnZvr06VI9ZR3V1dUljaV8RmUuRKPa1owyR0dS71Eles3ns88r95Qyv83MHnvsMWfm/e9/vzSWMnfXr1/vzKjP102bNjkzyn23c+dOqZ5ybbZs2SKNpZyHadOmOTPKe5eZtiZT1olmWr9T3ul9PkcTLfmOCAAAAAAAAACAY8DGNwAAAAAAAAAgVNj4BgAAAAAAAACEChvfAAAAAAAAAIBQYeMbAAAAAAAAABAqbHwDAAAAAAAAAEKFjW8AAAAAAAAAQKiw8Q0AAAAAAAAACJWoGkxJce+Rp6amSmMpud7eXmmsgoICZ6a2ttaZWb9+vVSvoaHBmVm+fLk0VltbmzMzefJkZ+bFF1+U6n3zm990ZrKzs52ZadOmSfWKioq8ZMzMRo0a5cxs2LDBmXnppZekeieddJIz09zcLI2l3DuxWEwaS6Heh4p4PO5trKGmfG7lWqgikYi3saJRdytWr0VnZ6czk5eXJ401ZcoUZ0bpP8o4ZmalpaXOjNIz3nzzTanePffc48wsWLBAGkvpURdffLEz8+CDD0r1lLnc2toqjbV582Zn5jOf+Ywz09jYKNVTjj0tLU0aayT1qPb2dmemr69PGisIAmdG6StmWs9Qrq3aV5Q12d69e6Wx8vPznRllvmVmZkr1MjIyvNRTzoGZ9vmmTp0qjaX0TmWO1tXVSfWU9bn6HC0rK3Nmxo4d68z89a9/lepNnDjRmTnuuOOksZR7NYyU+0C9/spYPt9TfVI+ozJHcnNzpXrKM6Srq8uZmTdvnlRPeQb/9re/lcbq7u52ZjZt2uTMqMeu9DKlJ5qZFRcXOzO+5rFKHcvn+0wyUO/zRL/HKcel7geceuqpzoyyz2SmzfGenh5nRl23V1VVOTMtLS3OjPqOo/S7N954QxpLOXZlPXnVVVdJ9ZTj2r9/vzSW8p66Y8cOZ0Z9t1DuL/Ve9dUX+RvfAAAAAAAAAIBQYeMbAAAAAAAAABAqbHwDAAAAAAAAAEKFjW8AAAAAAAAAQKiw8Q0AAAAAAAAACBU2vgEAAAAAAAAAocLGNwAAAAAAAAAgVNj4BgAAAAAAAACEChvfAAAAAAAAAIBQifocLBKJSLlYLObMFBQUSGPl5OQ4M9XV1c5McXGxVK+9vd2ZeeONN6Sxxo8f78zs27fPmTn55JOlert373Zmjj/+eGcmIyNDqldeXu7MKOfAzCwejzsz2dnZzszDDz8s1fvABz7gzLS2tkpjKfdFamqqM6PcN2o9VUrKyPmzMeUcqudGGUs9N8pYvjJmZn19fc7MzJkzpbH27NnjzOzcudOZqaurk+pNmDDBmVGeDdOnT5fqKefq8ccfl8aaOHGiM1NYWOjMLFmyRKpXWVnpzPT29kpjvfTSS87M/PnznZlJkyZJ9To6OpyZkdR7VEEQeMmYaXO3u7tbGkt5lilzSZ1vWVlZzoyyfjAzq6qqcmZ8fT4z7djz8/OdmfT0dKlec3OzM6M+19LS0pwZ5bhyc3OlesocVdc1f/nLX5wZpSeqxowZ48yo9+pI6mXKsaqfJxp1v2IqGbWmz7WvQrmfVMp5UPqKmX5PuYwbN07KZWZmOjPq+9KDDz7ozGzdutWZ+dvf/ibVU97plfWKmVlRUZEzo7zLqveE0l9HUu/xyWcv8Nnnlfe4kpISqd6sWbOcmf/7v/+TxlLm3JQpU5yZlpYWqV5PT48zM2rUKGdGeUc103qiugZUbNy40ZlR+piZtuZU9r/MtL1O5bja2tqkesp6Un1eKf1O8e7siAAAAAAAAACA0GLjGwAAAAAAAAAQKmx8AwAAAAAAAABChY1vAAAAAAAAAECosPENAAAAAAAAAAgVNr4BAAAAAAAAAKHCxjcAAAAAAAAAIFTY+AYAAAAAAAAAhErU52ApKdo+eiQScWaysrKksf761786M7FYzJkpKyuT6ilee+01KTdq1ChnJicnx5kpKSmR6hUXFzszhYWFzszy5culegUFBc5MbW2tNNaePXucmbS0NG/1ent7nRllHpuZBUEg5VxSU1O9jHMkYyn3zkgSjWotL9Hn2lfGzKyrq8uZUe5zM7O+vj5nRuljbW1tUr0XX3zRmTnppJOcmfT0dKne6NGjnRnlGWNmVlpa6sw899xzzszSpUuletOnT3dm1Dnz4Q9/2Jm58847nZnLLrtMqjd16lRnpr29XRprJInH485Md3e3NJZynyvPMTNtnijrO+XzmZnl5eU5M+PHj5fG2rVrlzOzfft2LxkzbV0zduxYZ0btUcozeNu2bdJYmzdvdmaUuaDMPTOzzMxMZ0aZC2bac0ZZ486YMUOqp6wn1ftLfTdKBsoaSV1HKZ87GddaZn6PXVlH5ebmOjN79+6V6j311FPOzMknn+zMqO9LSi9T54xyHpT+WllZKdWrrq52ZlpbW6WxioqKnBnlfVB9t3y3UubScPRcX71MefaYafNSve82bdrkzDQ3N3s5JjOtZyhrLfUZrKyj6uvrpbHOPPNMZ2b+/PnOjDpfJk6c6Mw0NDRIYyk9auPGjc7Mjh07pHqTJ092ZtR3PV/318hZjQEAAAAAAAAAIGDjGwAAAAAAAAAQKmx8AwAAAAAAAABChY1vAAAAAAAAAECosPENAAAAAAAAAAgVNr4BAAAAAAAAAKHCxjcAAAAAAAAAIFTY+AYAAAAAAAAAhErU52DxeFzKpaWlOTMpKdqefG9vrzOzc+dOZyY7O1uq193d7cyox15ZWenMZGZmOjPbtm2T6sViMWemsLDQmSkpKZHq5eXlOTNdXV3SWB0dHc5MbW2tMzN79mypns856oty/czMIpGIl8xIE42621lqaqo0lpJTx1KOS+Fzvil9zMzslFNOcWaqq6udmYqKCqleXV2dM7Nx40ZnZtSoUVI9pe+//PLL0lgnnXSSM/OHP/zBmSkrK5PqzZo1y5nJysqSxlJ6dU9PjzOzYcMGqZ5yrjo7O6Wx1PswGSjPO/XeVNY+fX190lhBEHjJqM+V5uZmZ6axsVEay5eCggIpd+655zozu3fvdmbUa6P0KGXdZqZ9xpycHGdG7a+lpaVe6plp97ky/5Q+Zqatt9RnsnLvjCTqfa7k1HPjax2lHnuinyu5ubnOjPIua2b22muvOTPKOkN9Fk2YMMGZmTp1qjTW5s2bnZnW1lZnRlmXmpm1tbU5M+pcUJ8hvvi6J8z059G7UaL7nbqns2XLFmdGXUcp+yfKukbdB1R62fbt250Zdd4q/e7UU0+VxrrkkkucGaWf7927V6rX0NDgzKSnp0tjKXtbyhpp3759Ur2ZM2c6M+3t7dJYvvat+BvfAAAAAAAAAIBQYeMbAAAAAAAAABAqbHwDAAAAAAAAAEKFjW8AAAAAAAAAQKiw8Q0AAAAAAAAACBU2vgEAAAAAAAAAocLGNwAAAAAAAAAgVNj4BgAAAAAAAACEChvfAAAAAAAAAIBQiQ5H0fT0dGemsbFRGislxb1339DQ4Mzs3LlTqqeorq6Wcs8//7wzM23aNGemq6tLqpedne3MTJkyxZnJzc2V6rW1tTkzfX190liK+vp6Z2bGjBnSWMq8ikQi0lhBEDgz8XhcGkuhHHssFvNWbyRRr5mSi0a19qmMlej5VlBQII01fvx4Z2bNmjXOzAsvvCDVKysrc2b27t3rzMyZM0eqN3nyZGfmr3/9qzTWunXrnBllzjzyyCNSPaWfn3zyydJYSj9QntudnZ1SvdTUVC8ZM/2+SAbKs1p9nnd3dzszap9XxlL6ilqvo6PDmWlvb5fGUs5XTk6OM9Pc3CzVmzp1qjNz2WWXOTN1dXVSPaVnqM8ihXKd1XVbb29vQsfySe0/Cp/ru5Ek0b3Z53PFJ6Wmcg8XFhZK9WbNmuXMPPfcc87M5s2bpXrnnnuuM3PiiSdKY40dO9aZ2bZtmzOjPj9aWlqcmfLycmks5TmjUOeoz/c4n8+QkUT53Mr7mUq5Zsp7kJm2FlEpz2Hl2MeMGSPVa21tdWbS0tKcmQ9+8INSvfe///3OzNy5c6WxiouLnRll/2vUqFFSvVdeecWZUfY5zbS91Z6eHmcmKytLqqfMmUSvE/gb3wAAAAAAAACAUGHjGwAAAAAAAAAQKmx8AwAAAAAAAABChY1vAAAAAAAAAECosPENAAAAAAAAAAgVNr4BAAAAAAAAAKHCxjcAAAAAAAAAIFTY+AYAAAAAAAAAhErU52ApKdo+eiwWc2ZSU1OlsSZMmODMdHZ2OjPPPvusVO+NN95wZnbu3CmN1djY6Mz09vY6M6NGjZLqFRcXOzNnnnmmM6Nem7q6Omemra1NGkvNubzvfe+Tcn19fc6Meh6U+Z5o6r06kkSj7namfm4lp9RTx1LmknrsSs+IRCLSWNu2bXNmNm7c6Mykp6dL9err652ZqqoqZ6aoqEiqd+GFFzozW7dulcZ6/vnnnRmlr2RlZUn1Jk+e7MyMHTtWGkuhzNGuri5pLGUux+NxaSy1DyeDnp4eZ0Z9XqjnR6Hcn0o9ta9kZ2c7M/n5+dJYylxS7ru0tDSp3h133OHMzJ4925nxOW+7u7u9jaXMvyAIvNVTz4OvnuHzvlHn+0iirGt8fm6fYylzRK3n87iUc9rQ0ODMvPzyy1I95R103759zsz69eulerm5uc7MwoULpbHKysqcGWVdo65FlPNeWFgojaWch/b2dmdGnXtK7/TZ75KF8rnV54rP57DyXFTqqXs6yrxU3yeU576yflUpe3dK5oQTTpDqTZs2zZkZP368NJbyjq3sWSm9x8yspaXFmXnttdeksZS+r8yr6dOnS/WUPqyuvX2tO8O3+wUAAAAAAAAAeFdj4xsAAAAAAAAAECpsfAMAAAAAAAAAQoWNbwAAAAAAAABAqLDxDQAAAAAAAAAIFTa+AQAAAAAAAAChwsY3AAAAAAAAACBU2PgGAAAAAAAAAIQKG98AAAAAAAAAgFCJysGoOxqJRKSxYrGYWtYpIyPDmSkoKHBmpk6dKtXLzs52ZsrLy6Wx4vG4M6Mcu2rRokXOzLhx45yZrVu3SvVaW1udmaamJmms9vZ2Z6asrMyZqaiokOp1dnY6Mykp/v7cKAgCZ0aZL+pY6r2ampoq5UYK9XP7vLZKTeXaKj3YzCwtLc2Z6e3tlcYaNWqUM3PiiSc6M5s2bZLq7dixw5lR7s0NGzZI9err652ZtrY2aazS0lJnZsuWLc5MTk6OVC8vL8+ZyczMlMZSKPNYvW+UnNp7lPk+kqif22eP6unp8TKOsj5SNTQ0SDnlPPhcv+7bt8+ZUY5d6a1mZt3d3c6MOmeUtUEixzkS6vonkfV83oPJwmefVyT6HPpc+/b19UljKc/0lpYWZ0Z9dx4zZowzo6yjpk2bJtU7//zznRl1/aqsa5T3dfX5oeRKSkqksZT9iI6ODmfGZz9Xn+3D0dOPltIzhqNHKc8M5R5Wr39WVpaXjJlZbm6uM1NUVOTMqD1KuVeUXj158mSp3oQJE5wZ9d1L6RnNzc3OTE1NjVRP2SdT31O3b9/uzHzkIx9xZsaOHSvVq6urc2Z83l+K8K3aAAAAAAAAAADvamx8AwAAAAAAAABChY1vAAAAAAAAAECosPENAAAAAAAAAAgVNr4BAAAAAAAAAKHCxjcAAAAAAAAAIFTY+AYAAAAAAAAAhAob3wAAAAAAAACAUIn6HCw1NdXbWPF4XMr19fU5M5mZmc5MQUGBVK++vt6Zyc7OlsaKRt2nf/bs2c7MzJkzpXrK9dm/f78z09LSItVrb2/3kjHTzvvUqVOdmdGjR3urF4lEpLESTbl3giCQxvJ5T4eNev1TUtx/vugrY6Zd/7S0NGmsMWPGODOTJ092Znbt2iXVa2trc2by8vKcmcrKSqlebm6uM3PGGWdIY/3f//2fM9Pd3e3MLF++XKqnzD/1/lV6ek9PjzNTU1Mj1VPmqPJ8NBtZPSojI8OZUe9z5R5W7icz7Xooc0T5fGZmsVjMSz0zbX2nUO5NM7OmpiZnRuk/p59+ulRPPS5flPtJXT+o63iFcl8oxzUc7ynqPZ0M1Gur8Pm5k7HPq2tApUcpz86ioiKpXmNjozPT1dXlzFx99dVSvU984hPOzHPPPSeNpRxXfn6+M5OTkyPVU66NsuY08zffk/UeTBaJfvf22eeVsdTPp9wHhYWF0lilpaXOTGdnpzOjvCeYaeu7sWPHOjPjx4+X6in3sNJ7zLQ1oLK3pa5xlfPe2toqjdXR0eHMnHfeec6Mz7WdOpavvhi+jggAAAAAAAAAeFdj4xsAAAAAAAAAECpsfAMAAAAAAAAAQoWNbwAAAAAAAABAqLDxDQAAAAAAAAAIFTa+AQAAAAAAAAChwsY3AAAAAAAAACBU2PgGAAAAAAAAAIRKdDiKpqS499tTU1Olsfbv3+/MNDY2OjO9vb1SvSAInBn12JXz0Nra6szEYjGpXltbmzPT1NTkzNTV1Un1WlpanJn29nZprPr6emfmrLPOcmbUaxOJRJwZ5fqZaXNGqadSjisej3urlyx8XrNo1N0a1bF8XluFMt+ys7OlsXp6epwZ5Z4aPXq0VG/nzp3OTHV1tTOTm5sr1TvllFOcmcLCQmms9PR0Z2bUqFHOzKRJk6R6+/btc2b+9re/SWNt2LDBmVGeH83NzVI95Xmr9mplvicLtWf4Gkvt88oaQskoay0zs66uLmdGWYuYab1a6Qdqn87IyHBmfvnLXzozCxculOqp94FCmQ8+7yefc1S5Pqyjjp3P85zosZRr5rMHK898M7OsrCxnRllndHd3S/WUd7RZs2Y5MxdffLFUb/369c6MsrYz054hylpEvc7KWri8vFwaS+mdPvc/lGdyot8/koXPz+3zGdzX1+fMKO9dZn6vv7KuUdZRPo9d6Yl5eXlSPWUvraOjQxpL2dtSxlKOSa2n7suNGTPGmfnQhz7kzKjnSrl31DWnr/UWf+MbAAAAAAAAABAqbHwDAAAAAAAAAEKFjW8AAAAAAAAAQKiw8Q0AAAAAAAAACBU2vgEAAAAAAAAAocLGNwAAAAAAAAAgVNj4BgAAAAAAAACEChvfAAAAAAAAAIBQYeMbAAAAAAAAABAqUZ+DxWIxKZeamurMRCIRaazOzk5nJggCL8dkZpaenu7MjB49WhpLOfa2tjZnprW1VaqnfMaenh4vx2RmVldX58y0tLRIY3V1dTkzc+bMcWb6+vqkesq5Uue7IiXF/WdQaj3l3lHnu8/PONSUz6R+buV6qD3K11jqscfjcW9jKXJycpyZcePGSWM1Nzc7M+vXr3dmlD5tpvVg9djPP/98Z0b5fLt27ZLq1dbWOjNbtmyRxnrzzTedmZqaGmfmAx/4gFQvIyPDmVGfMz7n8lBT5pv6jFLOT0NDgzSW8nzt6OjwkjHT1hnqWL29vc5MZmamM6M+65RzpfQM9fnhk/IsUp4fypraN6Wmr8+nUur5rjnUlHk5HHNXqalcD/WaKZS+YmZWXFzszCjP4OrqaqleYWGhM6M8q9Vzpaxr1H6+f/9+L/XUHqWsX6dOnSqNpTy7fe5/+LxXfd4XQ0051uF41/NFmd9mZo2Njc6Muq5JS0tzZpR7JSsrS6qnrNuU9zh1vazsk6nvHE1NTc6M0u/Uekpu79690ljLli1zZsaMGePMqM8in3tpvnrUyOl0AAAAAAAAAAAI2PgGAAAAAAAAAIQKG98AAAAAAAAAgFBh4xsAAAAAAAAAECpsfAMAAAAAAAAAQoWNbwAAAAAAAABAqLDxDQAAAAAAAAAIFTa+AQAAAAAAAAChElWDKSnuPXIlY2YWBIEzk5qaKo1VUlLizDQ3Nzsz6rHn5OQ4M8rnMzNrb293Zjo6OryMY6Z9xsbGRmemrq5Oqqec9zfffFMa6/jjj3dm5syZ48y0tbVJ9RTqnInH485MJBJJaD0kjnJtlX6nXn8ll5aWJo2lzCXl2LOzs6V65eXlzsyuXbucGbUndnZ2OjMTJ06UxkpPT/eS6e7ulurt37/fmXn99delsaqqqpyZpqYmZ+aiiy6S6imfUZ3vI4nyfO3p6ZHGqq2tdWaUOWKmPav7+vqcmWhUW1JWV1c7M+pzLDc315lRzun48eOlepdccokzc/LJJzsz6n3uk3JO1fUr3r3U3qysDdR3vUQ/D5T7ICMjQxpL6T/Ks3rChAlSve3btzszZWVlzozyzDfT+kosFpPGUq6z8vxQ+1hhYaEzM3bsWGkspabyjFTPlXrvKOj7h6aeG+VdT3kHUK+/8h6n7FmZab1MeY9TPp+ZthejHJOypjbTerCyDjYz27t3rzNTX1/vzLS0tEj1GhoanBllr9DM7JxzznFmlLW+z+exOlZvb6+fel5GAQAAAAAAAAAgSbDxDQAAAAAAAAAIFTa+AQAAAAAAAAChwsY3AAAAAAAAACBU2PgGAAAAAAAAAIQKG98AAAAAAAAAgFBh4xsAAAAAAAAAECpsfAMAAAAAAAAAQoWNbwAAAAAAAABAqETVYCwWc2ZSU1OlseLxuDMTBIE0VlFRkTOzb98+Z6ajo0Oql56e7sz09fVJYymfsbGx0ZnZvXu3VC8nJ8eZqampcWbq6uqkeu3t7c5MdXW1NNbVV1/tzGRlZTkz3d3dUj1lLiv3hColxf1nUJFIRBrL57Erx5UsfJ5DJaf2O+W40tLSvByTWq+np0caS+l3Sl9RxjEzGz16tDNTUlLizOTm5kr1CgoKnJnMzExprM7OTmdm7969zozaX5Wc0s/NzNavX+/MLF++3JmZNWuWVE95rkWj2vJEXSskg5dfftmZyc/Pl8Zqa2tzZtTrX1ZW5szMmTPHmZk3b55Ub8uWLc5MZWWlNJby3D/xxBOdmfe85z1SvcLCQmdG6QXqOlHt+wrl2aCsz1XKvamuMXy9N/hc0/g8V8lCmZfqOUz03FXqqddMyamfb//+/c5MRUWFM6O+v2zatMmZUfpYfX29VE9ZizQ0NEhjKe8mytpAeT6amU2dOtWZUdaJZlrfV/hc04yk9ZFKefcajndXpWZGRoYzo7xTmWnraLVHKTWVZ4O6t6BcQ+XY1Xuuq6vLS8ZM2y9Uer7aE7dv3+7MKM8PM7NFixY5My0tLc6M+n6mzAefe1uKkbOrBQAAAAAAAACAgI1vAAAAAAAAAECosPENAAAAAAAAAAgVNr4BAAAAAAAAAKHCxjcAAAAAAAAAIFTY+AYAAAAAAAAAhAob3wAAAAAAAACAUGHjGwAAAAAAAAAQKlE1GIlEvBVNTU11ZuLxuDRWenq6MzNt2jRnprOzU6rX0NDgzPT09Ehj+VJXV+dtLOXYgyCQxmptbXVmxo4dK431sY99zEs9dR6rnzGRUlK0P6dS7h11rFgsJuWSgdozEk05h0pG6ZtmZtGou613dHRIY2VnZzszubm5zkxBQYFUT7k/i4uLnZmamhqpXnl5uTOjPhuqqqqcma1btzoz27Ztk+rt3LnTmdmzZ480VmZmpjOzatUqZ6arq0uqp85lxUjqUffff78zU1hYKI2lzMsVK1ZIY335y192Znye54kTJzozixcvlsZSntVKX+nt7ZXqtbW1STkXpU+baZ9PvTbKeVDWBuqzVqnnc62V6HrqOips1OufjOtolXJt29vbpbGUNdL06dOdmSeeeEKqN2XKFGdGeQbv2LFDqqest5R3ZzWnHHtfX59U773vfa+XemrO53NUuQ/fDffqwfjcs/IpLS3NmVHXgLt27XJmlD0yM61HKev7+vp6qZ5yfyp9paioSKrX0tLizKg9qrm52Zmpra31Mo6Z2b59+5yZb37zm9JYyru4cp19rn0S/Q737ly1AQAAAAAAAABCi41vAAAAAAAAAECosPENAAAAAAAAAAgVNr4BAAAAAAAAAKHCxjcAAAAAAAAAIFTY+AYAAAAAAAAAhAob3wAAAAAAAACAUGHjGwAAAAAAAAAQKlE1mJLi3iOPRCLexkpNTZXGCoLAmcnMzHRmiouLpXoNDQ3OTEtLizRWPB53ZvLz86WxFD09Pc6Mcg1jsZhUr7W11Zk588wzpbGU61NXV+fMqHNUySnz2Eybo8o5Ve8J5dj7+vqksUYSX+dZzSn3k5lZenq6M6P0AuXzqdra2qRcUVGRM6N8PiWj5goLC50ZpReo6uvrpVxVVZUzs3fvXmdm69atUr2amhovx2RmdskllzgzkydPdmaU56OZ1suUe8JMv6eTgTIv9+/fL42l3CsXXnihNJbSW7q7u50Z9Rnl8/mj1nRR1wbRqHvZrJxPn/1cPQfKveJz7aPewwq1pstw9Auf52GoKedZvRbqPZWMlGvW29vrrZ66JlM0Nzc7M5s2bXJm1LWPshZRx1LOQ1dXlzOTm5sr1ZsxY4Yzo/ZqX/Pd516Kz+dMskjWvqL0DOX5k5OTI9UrKytzZpR708yso6PDmVHmm7I+MjPLyMhwZmpra50Z9fMp69fq6mppLKWmsv+1ZcsWqV5JSYkzc8UVV0hjNTY2OjNpaWnOjLqO8tmjfPUy/sY3AAAAAAAAACBU2PgGAAAAAAAAAIQKG98AAAAAAAAAgFBh4xsAAAAAAAAAECpsfAMAAAAAAAAAQoWNbwAAAAAAAABAqLDxDQAAAAAAAAAIFTa+AQAAAAAAAAChwsY3AAAAAAAAACBUomowHo87M0EQeBsrFotJYyk1e3t7nZmcnBypXkFBgTMTjWqnVfmMPT09zkxra6tUT6HUa29v91Zv/vz5Uk65hgqfc1SljOWznnrvKHwe11CLRCLOTEqK9md9Si41NdXbWEpG+XwqdY4oNTMyMpwZ9Vx1dnY6M01NTc6M2i8aGxu91DMzq6urc2ba2tqcmebmZqnerl27nJnc3FxprFWrVjkzSt9Xr7My/9T5rtZMBso6o76+XhrryiuvdGaOP/54aSxlzqWlpUljKZRr5vO6Ks8x9dmg8PkM9jlWoinrLfXzKdcn0X1FnaM+751kMBzn0Bf12H2uyZR3tG3btjkzHR0dUr2uri4v9bq7u6V6ynujep2VnPK8mjt3rlTvuOOOc2b6+vqksXyu0XFsfD5X1LmrXH8lo+4h5efnOzPKO4eZtr7PzMx0ZrKysqR6ylpY6WPKO5yZNh/UsVpaWpwZ5Xyqa/3vfOc7zkxxcbE0VnV1tTPj87ntc03mC3/jGwAAAAAAAAAQKmx8AwAAAAAAAABChY1vAAAAAAAAAECosPENAAAAAAAAAAgVNr4BAAAAAAAAAKHCxjcAAAAAAAAAIFTY+AYAAAAAAAAAhAob3wAAAAAAAACAUImqwSAInJl4PC6NFYvFnJnU1FRpLKVma2urM5ObmyvVKywsdGbS0tKksbq6upwZ5VxlZ2dL9RoaGpyZ5uZmZ6a7u1uql5mZ6cxMnz5dGquzs1PKuSjn00yfywrl3lGOSxnHTDt29TyoNZNBSkpi/xxP7VGRSMRLPfVaKMelHpPSy9ra2pwZpa+Yme3du9eZ2blzpzOTk5Mj1VOeDbt27ZLGqqurc2ba29udGbXXVVZWOjNf+9rXpLHGjRvnzCifT70nfPLZq4ea8kxU1yJXXXWVM9PT0yONFY26l4I++0qi54ny+XxSzoPaz309P3zyeew+n6PK80pZd6u5jo4OaazGxkZnpqioSBprqCXj2tdMW9/5vFd8juVrbaDeK2VlZc5MX1+fM5Ofny/VmzBhgjOzZ88eaSzl/VI59hkzZkj1lJ6hPkeTkc+9lJHE5/ug2qOUdYbSE9VjLygocGaUvmJmVl1d7cwozzt1fygjI8OZUeau+vmUa6geu9J/qqqqnBmlb5qZrVq1yplR1hhm2nn3uR+VjPgb3wAAAAAAAACAUGHjGwAAAAAAAAAQKmx8AwAAAAAAAABChY1vAAAAAAAAAECosPENAAAAAAAAAAgVNr4BAAAAAAAAAKHCxjcAAAAAAAAAIFTY+AYAAAAAAAAAhAob3wAAAAAAAACAUIkO9wEkQiwWc2aysrKksdLS0pyZaFQ7rdnZ2c5MS0uLM9Pa2irVq6mpcWa6urq81Rs1apQzU1RUJI3V29vrzEQiES8ZM7OUFH9/JqTMv9TUVC/jvJsp11a9ruo88SUej3sbq6enx5lReo+Zdh7a2tqcGaWvmGk9Ki8vz5nJycmR6jU3NzszDQ0N0lj79u1zZrq7u52ZyspKqd7JJ5/szFx66aXSWE1NTc6M0qNweMraIDMzUxpLWRukp6dLYynPV+X6+3xuhp3a830+94MgSOg4ymdUz4My3zdu3OjMbNiwQaqn9P3a2lpprPr6emfmxRdflMZKBj6vf7I+V5S1j3pvKu9CSq9Wz7tyr1RXVzsz6ruech46OzulsZRcRkaGMzNp0iSpnnLsPtfnPvk8rpH07FaOdTje9X0dV19fn1Qv0ddf2a8pLCyU6in3eXt7uzOzd+9eqZ5yn3d0dEhjKe+8ytrgW9/6llRv9OjRzozSz838zfdE75H4NHI6HQAAAAAAAAAAAja+AQAAAAAAAAChwsY3AAAAAAAAACBU2PgGAAAAAAAAAIQKG98AAAAAAAAAgFBh4xsAAAAAAAAAECpsfAMAAAAAAAAAQoWNbwAAAAAAAABAqESCIAiG+yAAAAAAAAAAAPCFv/ENAAAAAAAAAAgVNr4BAAAAAAAAAKHCxjcAAAAAAAAAIFTY+AYAAAAAAAAAhAob3wAAAAAAAACAUGHjGwAAAAAAAAAQKmx8AwAAAAAAAABChY1vAAAAAAAAAECosPENAAAAAAAAAAiV/w+ZHsmgRIwMswAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1500x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize some predictions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select a few random test samples\n",
    "num_samples_to_show = 5\n",
    "indices = np.random.randint(0, X_test.shape[0], num_samples_to_show)\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i, idx in enumerate(indices):\n",
    "    plt.subplot(1, num_samples_to_show, i+1)\n",
    "    plt.imshow(X_test[idx].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"Pred: {y_pred[idx]}, True: {y_test[idx]}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c006823e-adb7-4d48-9956-da8c7dc99038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from itertools import product\n",
    "\n",
    "class LogisticRegressionTuner:\n",
    "    def __init__(self, X_train, X_test, y_train, y_test):\n",
    "        \"\"\"\n",
    "        Initialize tuner with training and testing data\n",
    "        \n",
    "        Parameters:\n",
    "        - X_train: Training features\n",
    "        - X_test: Testing features\n",
    "        - y_train: Training labels\n",
    "        - y_test: Testing labels\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "    \n",
    "    def cross_validate(self, learning_rate, num_iterations, n_splits=5):\n",
    "        \"\"\"\n",
    "        Perform cross-validation\n",
    "        \n",
    "        Parameters:\n",
    "        - learning_rate: Learning rate to test\n",
    "        - num_iterations: Number of iterations\n",
    "        - n_splits: Number of cross-validation splits\n",
    "        \n",
    "        Returns:\n",
    "        - Average cross-validation accuracy\n",
    "        \"\"\"\n",
    "        # Stratified K-Fold for maintaining class distribution\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        \n",
    "        cv_scores = []\n",
    "        \n",
    "        for train_index, val_index in skf.split(self.X_train, self.y_train):\n",
    "            # Split data\n",
    "            X_train_cv = self.X_train[train_index]\n",
    "            y_train_cv = self.y_train[train_index]\n",
    "            X_val_cv = self.X_train[val_index]\n",
    "            y_val_cv = self.y_train[val_index]\n",
    "            \n",
    "            # Train model\n",
    "            clf = CustomLogisticRegression(\n",
    "                learning_rate=learning_rate, \n",
    "                num_iterations=num_iterations,\n",
    "                num_classes=len(np.unique(y_train_cv))\n",
    "            )\n",
    "            clf.fit(X_train_cv, y_train_cv)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            cv_accuracy = clf.accuracy(X_val_cv, y_val_cv)\n",
    "            cv_scores.append(cv_accuracy)\n",
    "        \n",
    "        return np.mean(cv_scores)\n",
    "    \n",
    "    def grid_search(self):\n",
    "        \"\"\"\n",
    "        Perform grid search for hyperparameters\n",
    "        \n",
    "        Returns:\n",
    "        - Best hyperparameters and their performance\n",
    "        \"\"\"\n",
    "        # Hyperparameter grid\n",
    "        learning_rates = [0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "        iterations_list = [500, 1000, 1500, 2000]\n",
    "        \n",
    "        # Store results\n",
    "        results = []\n",
    "        \n",
    "        # Grid search\n",
    "        for lr, iterations in product(learning_rates, iterations_list):\n",
    "            # Cross-validation\n",
    "            cv_accuracy = self.cross_validate(lr, iterations)\n",
    "            \n",
    "            # Final model evaluation on test set\n",
    "            clf = CustomLogisticRegression(\n",
    "                learning_rate=lr, \n",
    "                num_iterations=iterations,\n",
    "                num_classes=len(np.unique(self.y_train))\n",
    "            )\n",
    "            clf.fit(self.X_train, self.y_train)\n",
    "            test_accuracy = clf.accuracy(self.X_test, self.y_test)\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'learning_rate': lr,\n",
    "                'iterations': iterations,\n",
    "                'cv_accuracy': cv_accuracy,\n",
    "                'test_accuracy': test_accuracy\n",
    "            })\n",
    "            \n",
    "            print(f\"LR: {lr}, Iterations: {iterations}\")\n",
    "            print(f\"CV Accuracy: {cv_accuracy * 100:.2f}%\")\n",
    "            print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\\n\")\n",
    "        \n",
    "        # Find best parameters\n",
    "        results_df = pd.DataFrame(results)\n",
    "        best_cv_row = results_df.loc[results_df['cv_accuracy'].idxmax()]\n",
    "        best_test_row = results_df.loc[results_df['test_accuracy'].idxmax()]\n",
    "        \n",
    "        print(\"Best by Cross-Validation:\")\n",
    "        print(best_cv_row)\n",
    "        print(\"\\nBest by Test Accuracy:\")\n",
    "        print(best_test_row)\n",
    "        \n",
    "        return results_df, best_cv_row, best_test_row\n",
    "    \n",
    "    def feature_importance(self):\n",
    "        \"\"\"\n",
    "        Analyze feature importance\n",
    "        \n",
    "        Returns:\n",
    "        - Feature importance scores\n",
    "        \"\"\"\n",
    "        # Train final model\n",
    "        clf = CustomLogisticRegression(\n",
    "            learning_rate=0.1, \n",
    "            num_iterations=1000,\n",
    "            num_classes=len(np.unique(self.y_train))\n",
    "        )\n",
    "        clf.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        # Compute feature importance based on absolute weight magnitudes\n",
    "        feature_importances = np.abs(clf.weights).mean(axis=1)\n",
    "        \n",
    "        # Sort features by importance\n",
    "        sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "        \n",
    "        # Visualize top 10 most important features\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.title(\"Top 10 Most Important Features\")\n",
    "        plt.bar(range(10), feature_importances[sorted_indices[:10]])\n",
    "        plt.xlabel(\"Feature Index\")\n",
    "        plt.ylabel(\"Importance Score\")\n",
    "        plt.show()\n",
    "        \n",
    "        return feature_importances, sorted_indices\n",
    "\n",
    "# Additional utility function for dimensionality reduction\n",
    "def apply_pca(X_train, X_test, n_components=50):\n",
    "    \"\"\"\n",
    "    Apply PCA for dimensionality reduction\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Training features\n",
    "    - X_test: Testing features\n",
    "    - n_components: Number of components to keep\n",
    "    \n",
    "    Returns:\n",
    "    - Reduced X_train, X_test\n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    \n",
    "    # Explained variance\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(explained_variance)\n",
    "    \n",
    "    # Visualize explained variance\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'bo-')\n",
    "    plt.title('Cumulative Explained Variance')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.show()\n",
    "    \n",
    "    return X_train_pca, X_test_pca\n",
    "\n",
    "# Main execution function\n",
    "def hyperparameter_tuning(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Comprehensive hyperparameter tuning pipeline\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Training features\n",
    "    - X_test: Testing features\n",
    "    - y_train: Training labels\n",
    "    - y_test: Testing labels\n",
    "    \n",
    "    Returns:\n",
    "    - Tuning results\n",
    "    \"\"\"\n",
    "    # Tuning with original features\n",
    "    tuner = LogisticRegressionTuner(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # Perform grid search\n",
    "    results_df, best_cv, best_test = tuner.grid_search()\n",
    "    \n",
    "    # Analyze feature importance\n",
    "    feature_importances, sorted_indices = tuner.feature_importance()\n",
    "    \n",
    "    # Optional: Apply PCA and retune\n",
    "    X_train_pca, X_test_pca = apply_pca(X_train, X_test)\n",
    "    \n",
    "    # Tuning with PCA-reduced features\n",
    "    pca_tuner = LogisticRegressionTuner(X_train_pca, X_test_pca, y_train, y_test)\n",
    "    pca_results_df, pca_best_cv, pca_best_test = pca_tuner.grid_search()\n",
    "    \n",
    "    return {\n",
    "        'original_results': results_df,\n",
    "        'best_cv': best_cv,\n",
    "        'best_test': best_test,\n",
    "        'pca_results': pca_results_df,\n",
    "        'pca_best_cv': pca_best_cv,\n",
    "        'pca_best_test': pca_best_test\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "# results = hyperparameter_tuning(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4580b6e4-fe05-4df3-81bc-95dbfe3bfd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After preprocessing your data\n",
    "results = hyperparameter_tuning(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a28610-c3a4-44a3-9298-e2e7dc98cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLogisticRegressionWithReg(CustomLogisticRegression):\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000, num_classes=None, reg_strength=0.01):\n",
    "        super().__init__(learning_rate, num_iterations, num_classes)\n",
    "        self.reg_strength = reg_strength  # L2 Regularization strength\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        unique_classes = np.unique(y)\n",
    "        self.num_classes = len(unique_classes)\n",
    "        num_features = X.shape[1]\n",
    "        self.weights = np.zeros((num_features, self.num_classes))\n",
    "        self.bias = np.zeros((1, self.num_classes))\n",
    "        Y_one_hot = self._one_hot_encode(y)\n",
    "        \n",
    "        for _ in range(self.num_iterations):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self._softmax(linear_model)\n",
    "            dw = (1 / X.shape[0]) * np.dot(X.T, (y_predicted - Y_one_hot)) + self.reg_strength * self.weights\n",
    "            db = (1 / X.shape[0]) * np.sum(y_predicted - Y_one_hot, axis=0, keepdims=True)\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec823b93-5fae-48cd-9dff-c4bb09388ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def tune_hyperparameters(X_train, y_train, X_test, y_test):\n",
    "    learning_rates = [0.01, 0.05, 0.1]\n",
    "    num_iterations_list = [500, 1000, 2000]\n",
    "    reg_strengths = [0.01, 0.1, 1.0]\n",
    "\n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "    best_params = None\n",
    "\n",
    "    for lr, num_iter, reg in product(learning_rates, num_iterations_list, reg_strengths):\n",
    "        print(f\"Training with learning_rate={lr}, num_iterations={num_iter}, reg_strength={reg}\")\n",
    "        model = CustomLogisticRegressionWithReg(learning_rate=lr, num_iterations=num_iter, reg_strength=reg)\n",
    "        model.fit(X_train, y_train)\n",
    "        accuracy = model.accuracy(X_test, y_test)\n",
    "        print(f\"Testing accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_model = model\n",
    "            best_accuracy = accuracy\n",
    "            best_params = (lr, num_iter, reg)\n",
    "\n",
    "    print(\"\\nBest Hyperparameters:\")\n",
    "    print(f\"Learning Rate: {best_params[0]}, Number of Iterations: {best_params[1]}, Regularization Strength: {best_params[2]}\")\n",
    "    print(f\"Best Testing Accuracy: {best_accuracy * 100:.2f}%\")\n",
    "    return best_model, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0788b2-77b9-461c-bf82-8cd6c6ed4d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "X_train, X_test, y_train, y_test = preprocess_sign_mnist(train_data, test_data)\n",
    "\n",
    "# Hyperparameter tuning with detailed output\n",
    "print(\"Starting hyperparameter tuning...\\n\")\n",
    "best_model, best_params = tune_hyperparameters(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "print(\"\\nEvaluating the best model with optimal hyperparameters...\")\n",
    "evaluate_model(best_model, X_test, y_test)\n",
    "\n",
    "# Print the final results\n",
    "print(\"\\nSummary of Best Model:\")\n",
    "print(f\"Best Hyperparameters:\\n  Learning Rate: {best_params[0]}\\n  Number of Iterations: {best_params[1]}\\n  Regularization Strength: {best_params[2]}\")\n",
    "print(f\"Best Testing Accuracy: {best_model.accuracy(X_test, y_test) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfddd51-0e28-4938-9c48-a9a26314ee31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fd1efd-d9b8-463a-9742-f79de754667b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
